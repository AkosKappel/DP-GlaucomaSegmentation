{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Interpretability and Explainability"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import albumentations as A\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules import *\n",
    "from networks import *\n",
    "from training import *\n",
    "from interpretability import *\n",
    "\n",
    "COORDS = 'polar'  # cartesian, polar\n",
    "ARCH = 'cascade'  # dual, cascade\n",
    "MODEL = 'ref'  # rau, ref, swin\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SIZE = 256 if MODEL != 'swin' else 224\n",
    "transform = A.Compose([\n",
    "    A.Resize(height=SIZE, width=SIZE, interpolation=cv.INTER_AREA),\n",
    "    A.Lambda(image=sharpen, p=1.0),\n",
    "    A.Lambda(image=polar_transform, mask=polar_transform),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "all_images = load_files_from_dir(['../data/DRISHTI/ROI/TestImages'])\n",
    "all_masks = load_files_from_dir(['../data/DRISHTI/ROI/TestMasks'])\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(len(all_images))\n",
    "all_images = [all_images[i] for i in indices]\n",
    "all_masks = [all_masks[i] for i in indices]\n",
    "\n",
    "loader = load_dataset(\n",
    "    all_images,\n",
    "    all_masks,\n",
    "    transform,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Sample data\n",
    "images, masks = next(iter(loader))\n",
    "images = images.float().to(device)\n",
    "masks = masks.long().to(device)\n",
    "\n",
    "# Load models\n",
    "path = rf\"..\\models\\{COORDS}\\{MODEL}\\binary.pth\"\n",
    "checkpoint = load_checkpoint(path, map_location=device)\n",
    "base_model = checkpoint['model']\n",
    "base_model = base_model.eval()\n",
    "\n",
    "path = rf\"..\\models\\{COORDS}\\{MODEL}\\{ARCH}.pth\"\n",
    "checkpoint = load_checkpoint(path, map_location=device)\n",
    "model = checkpoint['model']\n",
    "model = model.eval()\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "activation_maps = ActivationMaps(model, {\n",
    "    'conv1': model.encoder.en1,\n",
    "    'conv2': model.encoder.en2,\n",
    "    'conv3': model.encoder.en3,\n",
    "    'conv4': model.encoder.en4,\n",
    "    'conv5': model.encoder.en5,\n",
    "    'conv6': model.decoder.de1,\n",
    "    'conv7': model.decoder.de2,\n",
    "    'conv8': model.decoder.de3,\n",
    "    'conv9': model.decoder.de4,\n",
    "    'last': model.decoder.last,\n",
    "}, images)\n",
    "activations = activation_maps.get_activations()\n",
    "activation_maps.unregister_hooks()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot conv1 activations\n",
    "fig, ax = plt.subplots(4, 8, figsize=(16, 8))\n",
    "plt.suptitle('conv1 activations')\n",
    "ax = ax.ravel()\n",
    "for i in range(32):\n",
    "    ax[i].imshow(activations['conv1'][0][0][i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# activation_maps.show('conv1')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Grad-CAM (Gradient-weighted Class Activation Mapping)",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_layers = dict(model.named_modules())\n",
    "# print(model_layers.keys())\n",
    "target_layer = model_layers['decoder.de1.conv']\n",
    "\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "out = gradcam(images)\n",
    "print(out.shape)\n",
    "\n",
    "gradcam.show()\n",
    "gradcam.unregister_hooks()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
