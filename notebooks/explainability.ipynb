{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Interpretability and Explainability"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import albumentations as A\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules import *\n",
    "from networks import *\n",
    "from training import *\n",
    "from interpretability import *\n",
    "\n",
    "COORDS = 'polar'  # cartesian, polar\n",
    "ARCH = 'dual'  # dual, cascade\n",
    "MODEL = 'ref'  # rau, ref, swin\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SIZE = 256 if MODEL != 'swin' else 224\n",
    "transform = A.Compose([\n",
    "    A.Resize(height=SIZE, width=SIZE, interpolation=cv.INTER_AREA),\n",
    "    A.Lambda(image=sharpen, p=1.0),\n",
    "    A.Lambda(image=polar_transform, mask=polar_transform),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "all_images = load_files_from_dir(['../data/DRISHTI/ROI/TestImages'])\n",
    "all_masks = load_files_from_dir(['../data/DRISHTI/ROI/TestMasks'])\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(len(all_images))\n",
    "all_images = [all_images[i] for i in indices]\n",
    "all_masks = [all_masks[i] for i in indices]\n",
    "\n",
    "loader = load_dataset(\n",
    "    all_images,\n",
    "    all_masks,\n",
    "    transform,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Sample data\n",
    "images, masks = next(iter(loader))\n",
    "images = images.float().to(device)\n",
    "masks = masks.long().to(device)\n",
    "\n",
    "# Load models\n",
    "path = rf\"..\\models\\{COORDS}\\{MODEL}\\binary.pth\"\n",
    "checkpoint = load_checkpoint(path, map_location=device)\n",
    "base_model = checkpoint['model']\n",
    "base_model = base_model.eval()\n",
    "\n",
    "path = rf\"..\\models\\{COORDS}\\{MODEL}\\{ARCH}.pth\"\n",
    "checkpoint = load_checkpoint(path, map_location=device)\n",
    "model = checkpoint['model']\n",
    "model = model.eval()\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T20:53:22.400027Z",
     "start_time": "2024-04-21T20:53:22.339687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cascade_layers = {\n",
    "#     # 'Encoder block 1': model.encoder.en1,\n",
    "#     'Encoder block 2': model.encoder.en2,\n",
    "#     # 'Encoder block 3': model.encoder.en3,\n",
    "#     # 'Encoder block 4': model.encoder.en4,\n",
    "#     'Encoder block 5': model.encoder.en5,\n",
    "\n",
    "#     'Decoder block 1': model.decoder.de4,\n",
    "#     # 'Decoder block 2': model.decoder.de3,\n",
    "#     # 'Decoder block 3': model.decoder.de2,\n",
    "#     'Decoder block 4': model.decoder.de1,\n",
    "#     # 'Output': model.decoder.last,\n",
    "# \n",
    "#     # 'Side input 1': model.encoder.side1,\n",
    "#     # 'Side input 2': model.encoder.side2,\n",
    "#     # 'Side input 3': model.encoder.side3,\n",
    "# }\n",
    "\n",
    "layers = {\n",
    "    # 'Encoder block 1': model.encoder.en1,\n",
    "    # 'Encoder block 2': model.encoder.en2,\n",
    "    # 'Encoder block 3': model.encoder.en3,\n",
    "    # 'Encoder block 4': model.encoder.en4,\n",
    "    # 'Encoder block 5': model.encoder.en5,\n",
    "\n",
    "    # 'Decoder block 1': model.decoder.de4,\n",
    "    # 'Decoder block 2': model.decoder.de3,\n",
    "    # 'Decoder block 3': model.decoder.de2,\n",
    "    # 'Decoder block 4': model.decoder.de1,\n",
    "    # 'Output': model.decoder.last,\n",
    "\n",
    "    'OD Decoder block 1': model.decoder1.de4,\n",
    "    # 'OD Decoder block 2': model.decoder1.de3,\n",
    "    # 'OD Decoder block 3': model.decoder1.de2,\n",
    "    'OD Decoder block 4': model.decoder1.de1,\n",
    "    # 'OD Output': model.decoder1.last,\n",
    "\n",
    "    'OC Decoder block 1': model.decoder2.de4,\n",
    "    # 'OC Decoder block 2': model.decoder2.de3,\n",
    "    # 'OC Decoder block 3': model.decoder2.de2,\n",
    "    'OC Decoder block 4': model.decoder2.de1,\n",
    "    # 'OC Output': model.decoder2.last,\n",
    "\n",
    "    # 'Side input 1': model.encoder.side1,\n",
    "    # 'Side input 2': model.encoder.side2,\n",
    "    # 'Side input 3': model.encoder.side3,\n",
    "}\n",
    "\n",
    "activation_maps = ActivationMaps(model, layers, images)\n",
    "activations = activation_maps.get_activations()\n",
    "activation_maps.unregister_hooks()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvCBAM module activations saved in 'OD Decoder block 1' with shape=(1, 80, 32, 32)\n",
      "ConvCBAM module activations saved in 'OD Decoder block 4' with shape=(1, 32, 256, 256)\n",
      "ConvCBAM module activations saved in 'OC Decoder block 1' with shape=(1, 80, 32, 32)\n",
      "ConvCBAM module activations saved in 'OC Decoder block 4' with shape=(1, 32, 256, 256)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for name in layers:\n",
    "    activation_maps.show(name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Plot selected activations for the thesis visualization\n",
    "fig, ax = plt.subplots(4, 8, figsize=(16, 8))\n",
    "plt.suptitle('conv1 activations')\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(32):\n",
    "    ax[i].imshow(activations['conv1'][0][0][i])\n",
    "    ax[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Grad-CAM (Gradient-weighted Class Activation Mapping)",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_layers = dict(model.named_modules())\n",
    "# print(model_layers.keys())\n",
    "target_layer = model_layers['decoder.de1.conv']\n",
    "\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "out = gradcam(images)\n",
    "print(out.shape)\n",
    "\n",
    "gradcam.show()\n",
    "gradcam.unregister_hooks()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
