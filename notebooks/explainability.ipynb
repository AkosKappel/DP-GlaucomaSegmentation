{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Interpretability and Explainability"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare model and data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from collections import defaultdict\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from interpretability import *\n",
    "from networks import *\n",
    "from utils import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMAGE_DIR = '../data/ORIGA/Images_Cropped'\n",
    "MASK_DIR = '../data/ORIGA/Masks_Cropped'\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 128, 128\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.pytorch.ToTensorV2(),\n",
    "])\n",
    "\n",
    "train_loader, val_loader, test_loader = load_origa(\n",
    "    IMAGE_DIR, MASK_DIR, 0.7, 0.15, 0.15, test_transform, test_transform, test_transform, batch_size=4,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Unet(in_channels=3, out_channels=3, features=[32, 64, 128, 256, 512]).to(DEVICE)\n",
    "checkpoint = torch.load('../checkpoints/best-multiclass-Unet-model.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "images, masks = next(iter(test_loader))\n",
    "images = images.float().to(DEVICE)\n",
    "masks = masks.long().to(DEVICE)\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_activation(name):\n",
    "    def hook(model, inputs, outputs):\n",
    "        activation[name] = outputs.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "activation = defaultdict(list)\n",
    "model.encoder.conv1.register_forward_hook(get_activation('conv1'))\n",
    "model.encoder.conv2.register_forward_hook(get_activation('conv2'))\n",
    "model.encoder.conv3.register_forward_hook(get_activation('conv3'))\n",
    "model.encoder.conv4.register_forward_hook(get_activation('conv4'))\n",
    "model.encoder.conv5.register_forward_hook(get_activation('conv5'))\n",
    "model.decoder.conv1.register_forward_hook(get_activation('conv6'))\n",
    "model.decoder.conv2.register_forward_hook(get_activation('conv7'))\n",
    "model.decoder.conv3.register_forward_hook(get_activation('conv8'))\n",
    "model.decoder.conv4.register_forward_hook(get_activation('conv9'))\n",
    "model.decoder.final.register_forward_hook(get_activation('conv10'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(images)\n",
    "\n",
    "# Plot conv1 activations\n",
    "fig, ax = plt.subplots(4, 8, figsize=(16, 8))\n",
    "plt.suptitle('conv1 activations')\n",
    "ax = ax.ravel()\n",
    "for i in range(32):\n",
    "    ax[i].imshow(activation['conv1'][0][i].cpu().numpy())\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot conv5 activations\n",
    "fig, ax = plt.subplots(4, 8, figsize=(16, 8))\n",
    "plt.suptitle('conv5 activations')\n",
    "ax = ax.ravel()\n",
    "for i in range(32):\n",
    "    ax[i].imshow(activation['conv5'][0][i].cpu().numpy())\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot conv9 activations\n",
    "fig, ax = plt.subplots(4, 8, figsize=(16, 8))\n",
    "plt.suptitle('conv9 activations')\n",
    "ax = ax.ravel()\n",
    "for i in range(32):\n",
    "    ax[i].imshow(activation['conv9'][0][i].cpu().numpy())\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grad-CAM (Gradient-weighted Class Activation Mapping)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_layers = dict(model.named_modules())\n",
    "# print(model_layers.keys())\n",
    "target_layer = model_layers['decoder.conv2.conv']\n",
    "\n",
    "image = images[3:4]\n",
    "mask = masks[3:4]\n",
    "\n",
    "outputs = model(image)\n",
    "probs = torch.softmax(outputs, dim=1)\n",
    "preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "heatmap = gradcam(image, class_idx=2)\n",
    "heatmap = heatmap.detach().cpu().numpy()\n",
    "heatmap = cv.resize(heatmap, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "heatmap = np.uint8(255 * heatmap / np.max(heatmap))\n",
    "\n",
    "plt.imshow(heatmap)\n",
    "\n",
    "image = image.squeeze().detach().cpu().numpy().transpose(1, 2, 0) / 255\n",
    "mask = mask.squeeze().detach().cpu().numpy()\n",
    "pred = preds.squeeze().detach().cpu().numpy()\n",
    "\n",
    "overlay = cv.applyColorMap(heatmap, cv.COLORMAP_JET)\n",
    "overlay = cv.cvtColor(overlay, cv.COLOR_BGR2RGB) / 255\n",
    "\n",
    "alpha = 0.5\n",
    "combined = alpha * overlay + (1 - alpha) * image\n",
    "\n",
    "_, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(mask)\n",
    "ax[2].imshow(pred)\n",
    "ax[3].imshow(overlay)\n",
    "ax[4].imshow(combined)\n",
    "plt.show()\n",
    "\n",
    "gradcam.unregister_hooks()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Guided Backpropagation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Guided Grad-CAM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Occlusion Sensitivity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def occlusion_analysis(model, input_image, target_class, patch_size=32, stride=16):\n",
    "    model.eval()\n",
    "    image = input_image.clone()\n",
    "    height, width = image.size(2), image.size(3)\n",
    "\n",
    "    # Calculate the number of patches in both dimensions\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    heatmap = torch.zeros((num_patches_h, num_patches_w), dtype=torch.float32)\n",
    "\n",
    "    # Get the model's prediction without occlusion (baseline)\n",
    "    with torch.no_grad():\n",
    "        baseline_output = model(image)\n",
    "\n",
    "    # Calculate the probability of the target class in the baseline output\n",
    "    baseline_prob = F.softmax(baseline_output, dim=1)[0, target_class]\n",
    "\n",
    "    # Iterate over each patch\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            # Occlude the patch\n",
    "            image[0, :, i * stride:i * stride + patch_size, j * stride:j * stride + patch_size] = 0.0\n",
    "\n",
    "            # Get the model's prediction after occlusion\n",
    "            with torch.no_grad():\n",
    "                output = model(image)\n",
    "\n",
    "            occluded_prob = F.softmax(output, dim=1)[0, target_class]\n",
    "\n",
    "            # Update the heatmap with the confidence score\n",
    "            diff = torch.abs(baseline_prob - occluded_prob)\n",
    "            heatmap[i, j] = torch.mean(diff).item()\n",
    "\n",
    "            # _, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            # ax[0].imshow(image.squeeze().permute(1, 2, 0).cpu().numpy() / 255)\n",
    "            # ax[1].imshow(occluded_prob.squeeze().cpu().numpy())\n",
    "            # ax[2].imshow(heatmap.cpu().numpy())\n",
    "            # plt.show()\n",
    "\n",
    "            # Reset the occluded patch\n",
    "            image[0, :, i * stride:i * stride + patch_size, j * stride:j * stride + patch_size] = \\\n",
    "                input_image[0, :, i * stride:i * stride + patch_size, j * stride:j * stride + patch_size].clone()\n",
    "\n",
    "    # invert the heatmap values to obtain the saliency map\n",
    "    # heatmap = torch.max(heatmap) - heatmap\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "# Assuming you have loaded the trained model as 'model' and the input image as 'input_image'\n",
    "# and 'target_class' is the class index for which you want to perform occlusion analysis\n",
    "target_class = 2\n",
    "\n",
    "# Convert the input image to a tensor and normalize it (assuming the image is in RGB format)\n",
    "input_image = cv.imread(r\"C:\\Users\\ASUS\\PycharmProjects\\DP-GlaucomaSegmentation\\data\\ORIGA\\Images_Cropped\\049.jpg\")\n",
    "input_image = cv.cvtColor(input_image, cv.COLOR_BGR2RGB)\n",
    "input_image = cv.resize(input_image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "input_image = torch.tensor(input_image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "input_image = input_image.to(DEVICE)\n",
    "\n",
    "# Perform occlusion analysis for the target class\n",
    "heatmap = occlusion_analysis(model, input_image, target_class)\n",
    "\n",
    "# Upscale the heatmap to the original image size for visualization\n",
    "upscale_factor_h = input_image.size(2) // heatmap.size(0)\n",
    "upscale_factor_w = input_image.size(3) // heatmap.size(1)\n",
    "heatmap_upscaled = F.interpolate(heatmap.unsqueeze(0).unsqueeze(0), size=(input_image.size(2), input_image.size(3)),\n",
    "                                 mode='bicubic', align_corners=False).squeeze().numpy()\n",
    "\n",
    "# Normalize the heatmap values to the range [0, 255] for visualization\n",
    "heatmap_upscaled = (heatmap_upscaled - heatmap_upscaled.min()) / (heatmap_upscaled.max() - heatmap_upscaled.min())\n",
    "heatmap_upscaled = np.uint8(heatmap_upscaled * 255)\n",
    "\n",
    "# Apply the heatmap as an overlay on the original image\n",
    "overlayed_image = cv.applyColorMap(heatmap_upscaled, cv.COLORMAP_JET)\n",
    "overlayed_image = cv.cvtColor(overlayed_image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the result\n",
    "_, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].imshow(input_image.squeeze().permute(1, 2, 0).cpu().numpy() / 255)\n",
    "ax[1].imshow(input_image.squeeze().permute(1, 2, 0).cpu().numpy() / 255)\n",
    "ax[1].imshow(overlayed_image, alpha=0.5)\n",
    "ax[2].imshow(heatmap_upscaled)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Salience Maps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = images[1:2]\n",
    "mask = masks[1:2]\n",
    "\n",
    "image.requires_grad = True\n",
    "output = model(image)\n",
    "\n",
    "output_scalar = output.sum()\n",
    "\n",
    "model.zero_grad()\n",
    "output_scalar.backward()\n",
    "\n",
    "saliency_map = image.grad.abs().squeeze(0).cpu()\n",
    "saliency_map = saliency_map / saliency_map.max()\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].imshow(image.detach().squeeze(0).permute(1, 2, 0).cpu() / 255.0)\n",
    "ax[1].imshow(mask.squeeze(0).cpu())\n",
    "ax[2].imshow(saliency_map.permute(1, 2, 0))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the SaliencyMap class\n",
    "class SaliencyMap:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.gradients = None\n",
    "        self.register_hook()\n",
    "\n",
    "    def register_hook(self):\n",
    "        def hook_fn(module, grad_input, grad_output):\n",
    "            self.gradients = grad_input[0]\n",
    "\n",
    "        self.model.register_full_backward_hook(hook_fn)\n",
    "\n",
    "    def generate(self, inputs, class_idx=None):\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        inputs.requires_grad = True\n",
    "        outputs = self.model(inputs)\n",
    "\n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        onehot = torch.zeros(outputs.size(), dtype=torch.float32, device=inputs.device)\n",
    "        onehot.scatter_(1, class_idx.unsqueeze(1), 1.0)\n",
    "\n",
    "        outputs.backward(gradient=onehot, retain_graph=True)\n",
    "        saliency_map = torch.abs(self.gradients[0]).max(dim=0)[0]\n",
    "\n",
    "        return saliency_map\n",
    "\n",
    "\n",
    "# Initialize SaliencyMap\n",
    "saliency_map = SaliencyMap(model)\n",
    "\n",
    "# Load an input image for visualization (replace 'path_to_input_image.jpg' with the path to your image)\n",
    "input_image = cv.imread(r\"C:\\Users\\ASUS\\PycharmProjects\\DP-GlaucomaSegmentation\\data\\ORIGA\\Images_Cropped\\049.jpg\")\n",
    "input_image = cv.cvtColor(input_image, cv.COLOR_BGR2RGB)\n",
    "input_image = cv.resize(input_image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "input_image = torch.tensor(input_image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "input_image = input_image.to(DEVICE)\n",
    "\n",
    "# Compute the saliency map\n",
    "saliency_map_output = saliency_map.generate(input_image)\n",
    "\n",
    "# Normalize the saliency map for visualization\n",
    "saliency_map_output = (saliency_map_output - saliency_map_output.min()) / (\n",
    "        saliency_map_output.max() - saliency_map_output.min())\n",
    "saliency_map_output = saliency_map_output.squeeze().detach().cpu().numpy()\n",
    "\n",
    "# Display the original image and the saliency map\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(input_image.detach().squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "plt.title('Input Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(saliency_map_output, cmap='hot')\n",
    "plt.title('Saliency Map')\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention Maps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = AttentionUnet(in_channels=3, out_channels=3, features=[32, 64, 128, 256, 512]).to(DEVICE)\n",
    "checkpoint = torch.load('../checkpoints/best-attentionunet-multiclass-model.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "images, masks = next(iter(val_loader))\n",
    "images = images.float().to(DEVICE)\n",
    "masks = masks.long().to(DEVICE)\n",
    "# print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hook_fn(module, inputs, outputs):\n",
    "    global attention_map\n",
    "    attention_map = outputs.detach()\n",
    "\n",
    "\n",
    "attention_map = None\n",
    "model.decoder.ag4.psi.register_forward_hook(hook_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "probs = torch.softmax(outputs, dim=1)\n",
    "preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "# Visualize the attention map\n",
    "fig, ax = plt.subplots(4, 3, figsize=(12, 12))\n",
    "ax = ax.ravel()\n",
    "for i in range(4):\n",
    "    ax[3 * i].imshow(images[i].cpu().numpy().transpose(1, 2, 0) / 255.0)\n",
    "    ax[3 * i + 1].imshow(masks[i].cpu().numpy())\n",
    "    ax[3 * i + 2].imshow(attention_map[i, 0].cpu().numpy())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
