{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Glaucoma Segmentation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "from networks import *\n",
    "from training import *\n",
    "from utils import *\n",
    "\n",
    "# prepare_origa_dataset('../data/ORIGA')\n",
    "# prepare_drishti_dataset('../data/DRISHTI')\n",
    "# prepare_rimone_dataset('../data/RIMONE', 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMAGE_DIR = '../data/Kaggle-ORIGA/Images_CenterNet_Cropped'\n",
    "MASK_DIR = '../data/Kaggle-ORIGA/Masks_CenterNet_Cropped'\n",
    "LOGS_DIR = '../logs/'\n",
    "CHECKPOINT_DIR = '../checkpoints/'\n",
    "\n",
    "NETWORK_NAME = 'refunet3+cbam'  # raunet++, refunet3+cbam, swinunet\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS_FUNCTION = 'combo'\n",
    "ARCHITECTURE = 'binary'  # multiclass, multilabel, binary, dual, cascade\n",
    "BINARY_TARGET_CLASSES = [1, 2]\n",
    "SCHEDULER = 'plateau'\n",
    "SCALER = 'none'\n",
    "DATASET = 'ORIGA'\n",
    "BASE_CASCADE_MODEL = ''\n",
    "\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 128, 128\n",
    "# IMAGE_HEIGHT, IMAGE_WIDTH = 224, 224\n",
    "IN_CHANNELS, OUT_CHANNELS = 3, 1\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "LAYERS = [16, 32, 48, 64, 80]\n",
    "CLASS_WEIGHTS = None\n",
    "SET_SIZES = [0.8, 0.1, 0.1]\n",
    "DROPOUT_2D = 0.2\n",
    "EARLY_STOPPING_PATIENCE = 11\n",
    "LOG_INTERVAL = 5\n",
    "SAVE_INTERVAL = 10\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "USE_WANDB = False\n",
    "POLAR_TRANSFORM = True\n",
    "MULTI_SCALE_INPUT = False\n",
    "DEEP_SUPERVISION = False\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PIN_MEMORY = True if DEVICE == 'cuda' else False\n",
    "\n",
    "POSTPROCESSING = [\n",
    "    to_numpy,\n",
    "    unpack,\n",
    "    lambda x: fill_holes(x, binary=True),\n",
    "    lambda x: keep_largest_component(x, binary=True),\n",
    "    lambda x: dilate(x, kernel_size=5, iterations=1),\n",
    "    pack,\n",
    "    to_tensor,\n",
    "]\n",
    "\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f'''CONFIGURATION:\n",
    "    PyTorch version: {torch.__version__}\n",
    "    NumPy version: {np.__version__}\n",
    "    OpenCV version: {cv.__version__}\n",
    "\n",
    "    Network: {NETWORK_NAME}\n",
    "    Architecture: {ARCHITECTURE}\n",
    "    Optimizer: {OPTIMIZER}\n",
    "    Loss function: {LOSS_FUNCTION}\n",
    "    Scheduler: {SCHEDULER}\n",
    "    Scaler: {SCALER}\n",
    "    Using device: {DEVICE}\n",
    "\n",
    "    Dataset: {DATASET}\n",
    "    Dataset proportions: {SET_SIZES}\n",
    "    Image directory: {IMAGE_DIR}\n",
    "    Mask directory: {MASK_DIR}\n",
    "    Input image height & width: {IMAGE_HEIGHT}x{IMAGE_WIDTH}\n",
    "    Number of input channels: {IN_CHANNELS}\n",
    "    Number of output channels: {OUT_CHANNELS}\n",
    "\n",
    "    Layers: {LAYERS}\n",
    "    Batch size: {BATCH_SIZE}\n",
    "    Learning rate: {LEARNING_RATE}\n",
    "    Epochs: {EPOCHS}\n",
    "    Class weights: {CLASS_WEIGHTS}\n",
    "    Dropout: {DROPOUT_2D}\n",
    "    Early stopping patience: {EARLY_STOPPING_PATIENCE}\n",
    "\n",
    "    Save interval: {SAVE_INTERVAL}\n",
    "    Log interval: {LOG_INTERVAL}\n",
    "    Number of workers: {NUM_WORKERS}\n",
    "    Pin memory: {PIN_MEMORY}\n",
    "\n",
    "    Weight & Biases: {USE_WANDB}\n",
    "    Polar transform: {POLAR_TRANSFORM}\n",
    "    Multi-scale input: {MULTI_SCALE_INPUT}\n",
    "    Deep supervision: {DEEP_SUPERVISION}''')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polar_transform_partial = partial(polar_transform, radius_ratio=1.0)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, interpolation=cv.INTER_AREA),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=1.0),\n",
    "    A.CLAHE(p=1.0, clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    # A.GridDistortion(p=0.5, border_mode=cv.BORDER_CONSTANT),\n",
    "    # A.MedianBlur(p=0.5),\n",
    "    # A.RandomToneCurve(p=0.5),\n",
    "    # A.MultiplicativeNoise(p=0.5),\n",
    "    # A.Lambda(image=sharpen, p=1.0),\n",
    "    A.Lambda(image=polar_transform_partial, mask=polar_transform_partial) if POLAR_TRANSFORM else A.Lambda(),\n",
    "    # A.Lambda(image=keep_gray_channel),\n",
    "    # A.Lambda(image=keep_red_channel),\n",
    "    # A.Lambda(image=keep_green_channel),\n",
    "    # A.Lambda(image=keep_blue_channel),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, interpolation=cv.INTER_AREA),\n",
    "    A.CLAHE(p=1.0, clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True),\n",
    "    A.Lambda(image=polar_transform_partial, mask=polar_transform_partial) if POLAR_TRANSFORM else A.Lambda(),\n",
    "    # A.Lambda(image=keep_gray_channel),\n",
    "    # A.Lambda(image=keep_red_channel),\n",
    "    # A.Lambda(image=keep_green_channel),\n",
    "    # A.Lambda(image=keep_blue_channel),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "train_loader, val_loader, test_loader = load_dataset(\n",
    "    [IMAGE_DIR], [MASK_DIR],\n",
    "    None, None, *SET_SIZES,\n",
    "    train_transform, val_transform, val_transform,\n",
    "    BATCH_SIZE, PIN_MEMORY, NUM_WORKERS,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = None\n",
    "binary_model = None\n",
    "hist = None\n",
    "\n",
    "if NETWORK_NAME == 'raunet++':\n",
    "    model = RAUnetPlusPlus(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=LAYERS,\n",
    "        multi_scale_input=MULTI_SCALE_INPUT, deep_supervision=DEEP_SUPERVISION, dropout=DROPOUT_2D,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'refunet3+cbam':\n",
    "    model = RefUnet3PlusCBAM(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=LAYERS,\n",
    "        multi_scale_input=MULTI_SCALE_INPUT, dropout=DROPOUT_2D,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'swinunet':\n",
    "    model = SwinUnet(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, img_size=224, patch_size=4,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'dual-raunet++':\n",
    "    model = DualRAUnetPlusPlus(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=LAYERS,\n",
    "        multi_scale_input=MULTI_SCALE_INPUT, deep_supervision=DEEP_SUPERVISION, dropout=DROPOUT_2D,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'dual-refunet3+cbam':\n",
    "    model = DualRefUnet3PlusCBAM(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=LAYERS,\n",
    "        multi_scale_input=MULTI_SCALE_INPUT, dropout=DROPOUT_2D,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'dual-swinunet':\n",
    "    model = DualSwinUnet(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, img_size=224, patch_size=4,\n",
    "    )\n",
    "\n",
    "assert model is not None, 'Invalid network name'\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "init_model_weights(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = ComboLoss(num_classes=OUT_CHANNELS if ARCHITECTURE == 'multiclass' else 1, class_weights=CLASS_WEIGHTS)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, verbose=True)\n",
    "scaler = None\n",
    "\n",
    "if BASE_CASCADE_MODEL:\n",
    "    checkpoint = load_checkpoint(BASE_CASCADE_MODEL)\n",
    "    binary_model = checkpoint['model']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if ARCHITECTURE == 'multiclass':\n",
    "    hist = train_multiclass(\n",
    "        model, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "        save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "        save_best_model=True, plot_examples='none', show_plots=False,\n",
    "        inverse_transform=undo_polar_transform if POLAR_TRANSFORM else None,\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'multilabel':\n",
    "    hist = train_multilabel(\n",
    "        model, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "        save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "        save_best_model=True, plot_examples='none', show_plots=False,\n",
    "        inverse_transform=undo_polar_transform if POLAR_TRANSFORM else None,\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'binary':\n",
    "    hist = train_binary(\n",
    "        model, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "        save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "        save_best_model=True, plot_examples='none', show_plots=False, target_ids=BINARY_TARGET_CLASSES,\n",
    "        inverse_transform=undo_polar_transform if POLAR_TRANSFORM else None,\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'cascade':\n",
    "    assert binary_model is not None, 'Base model not specified'\n",
    "    hist = train_cascade(\n",
    "        binary_model, model, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "        save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "        save_best_model=True, plot_examples='none', show_plots=False, postprocess=POSTPROCESSING,\n",
    "        inverse_transform=undo_polar_transform if POLAR_TRANSFORM else None,\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'dual':\n",
    "    hist = train_dual(\n",
    "        model, criterion, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "        save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "        save_best_model=True, plot_examples='none', show_plots=False,\n",
    "        inverse_transform=undo_polar_transform if POLAR_TRANSFORM else None,\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_history(hist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if ARCHITECTURE == 'multiclass':\n",
    "    results = evaluate('multiclass', model, test_loader, criterion, DEVICE)\n",
    "\n",
    "if ARCHITECTURE == 'multilabel':\n",
    "    results = evaluate('multilabel', model, test_loader, criterion, DEVICE)\n",
    "\n",
    "if ARCHITECTURE == 'binary':\n",
    "    results = evaluate('binary', model, test_loader, criterion, DEVICE, class_ids=BINARY_TARGET_CLASSES)\n",
    "\n",
    "if ARCHITECTURE == 'dual':\n",
    "    results = evaluate('dual', model, test_loader, criterion, DEVICE)\n",
    "\n",
    "if ARCHITECTURE == 'cascade':\n",
    "    results = evaluate('cascade', model, test_loader, criterion, DEVICE, model0=binary_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if ARCHITECTURE == 'multiclass':\n",
    "    plot_results_from_loader(\n",
    "        'multiclass', test_loader, model, DEVICE, n_samples=4,\n",
    "        save_path=f'{LOGS_DIR}/evaluation.png',\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'multilabel':\n",
    "    plot_results_from_loader(\n",
    "        'multilabel', test_loader, model, DEVICE, n_samples=4,\n",
    "        save_path=f'{LOGS_DIR}/evaluation.png',\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'binary':\n",
    "    plot_results_from_loader(\n",
    "        'binary', test_loader, model, DEVICE, n_samples=4,\n",
    "        save_path=f'{LOGS_DIR}/evaluation.png', class_ids=BINARY_TARGET_CLASSES,\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'dual':\n",
    "    plot_results_from_loader(\n",
    "        'dual', test_loader, model, DEVICE,\n",
    "        n_samples=4, save_path=f'{LOGS_DIR}/evaluation.png',\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'cascade':\n",
    "    plot_results_from_loader(\n",
    "        'cascade', test_loader, model, DEVICE, n_samples=4,\n",
    "        save_path=f'{LOGS_DIR}/evaluation.png', model0=binary_model,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Work in progress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO\n",
    "WEIGHT_DECAY = 1e-4\n",
    "model = SwinUnet(in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, img_size=224, patch_size=4).to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = ComboLoss(OUT_CHANNELS)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1, verbose=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs, labels = inputs.float().to(DEVICE), labels.long().to(DEVICE)\n",
    "\n",
    "        # Binarize labels\n",
    "        labels[labels > 0] = 1\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update training loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Calculate average loss over an epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            inputs, labels = inputs.float().to(DEVICE), labels.long().to(DEVICE)\n",
    "\n",
    "            # Binarize labels\n",
    "            labels[labels > 0] = 1\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update validation loss\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Calculate average loss over an epoch\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(model, '.models/swinunet.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show some examples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.float().to(DEVICE), labels.long().to(DEVICE)\n",
    "\n",
    "        # Binarize labels\n",
    "        labels[labels > 0] = 1\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        images = inputs.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        masks = labels.detach().cpu().numpy()\n",
    "        preds = outputs.detach().cpu().numpy().squeeze()\n",
    "        break\n",
    "\n",
    "for img, mask, pred in zip(images, masks, preds):\n",
    "    img = (img / img.max() * 255).astype(np.uint8)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
