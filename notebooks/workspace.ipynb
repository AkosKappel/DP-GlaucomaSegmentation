{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Glaucoma Segmentation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import defaultdict\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from networks import *\n",
    "from training import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMAGE_DIR = '../data/ORIGA/Images_Cropped'\n",
    "MASK_DIR = '../data/ORIGA/Masks_Cropped'\n",
    "LOGS_DIR = '../logs/'\n",
    "CHECKPOINT_DIR = '../checkpoints/'\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 128, 128\n",
    "in_channels = 3\n",
    "out_channels = 1\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 3\n",
    "LAYERS = [32, 64, 128, 256, 512]\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "LOG_INTERVAL = 1\n",
    "SAVE_INTERVAL = 10\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "PIN_MEMORY = True if DEVICE == 'cuda' else False\n",
    "USE_WANDB = False\n",
    "DEEP_SUPERVISION = False\n",
    "\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f'''CONFIGURATION:\n",
    "    PyTorch version: {torch.__version__}\n",
    "    Using device: {DEVICE}\n",
    "    Image directory: {IMAGE_DIR}\n",
    "    Mask directory: {MASK_DIR}\n",
    "    Input image height & width: {IMAGE_HEIGHT}x{IMAGE_WIDTH}\n",
    "    Number of input channels: {in_channels}\n",
    "    Number of output channels: {out_channels}\n",
    "    Layers: {LAYERS}\n",
    "    Batch size: {BATCH_SIZE}\n",
    "    Learning rate: {LEARNING_RATE}\n",
    "    Epochs: {EPOCHS}\n",
    "    Early stopping patience: {EARLY_STOPPING_PATIENCE}\n",
    "    Save interval: {SAVE_INTERVAL}\n",
    "    Number of workers: {NUM_WORKERS}\n",
    "    Pin memory: {PIN_MEMORY}\n",
    "    Weight & Biases: {USE_WANDB}\n",
    "    Deep supervision: {DEEP_SUPERVISION}''')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def polar_transform(img, **kwargs):\n",
    "    height, width = img.shape[:2]\n",
    "    center = (width // 2, height // 2)\n",
    "    value = np.sqrt(((width / 2.0) ** 2.0) + ((height / 2.0) ** 2.0))\n",
    "    polar_img = cv.linearPolar(img, center, value, cv.WARP_FILL_OUTLIERS)\n",
    "    polar_img = cv.rotate(polar_img, cv.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    return polar_img\n",
    "\n",
    "\n",
    "def inverse_polar_transform(img, **kwargs):\n",
    "    img = cv.rotate(img, cv.ROTATE_90_CLOCKWISE)\n",
    "    height, width = img.shape[:2]\n",
    "    center = (width // 2, height // 2)\n",
    "    value = np.sqrt(((width / 2.0) ** 2.0) + ((height / 2.0) ** 2.0))\n",
    "    cartesian_img = cv.linearPolar(img, center, value, cv.WARP_INVERSE_MAP | cv.WARP_FILL_OUTLIERS)\n",
    "    return cartesian_img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_ds = OrigaDataset(IMAGE_DIR, MASK_DIR, os.listdir(IMAGE_DIR)[:1], A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=1.0),  # rotate by 0, 90, 180, or 270 degrees\n",
    "    # A.Rotate(limit=30, p=0.33, border_mode=cv.BORDER_CONSTANT),\n",
    "    # A.Lambda(image=polar_transform, mask=polar_transform),\n",
    "    # A.Lambda(image=inverse_polar_transform, mask=inverse_polar_transform),\n",
    "\n",
    "    # A.CLAHE(p=0.5),\n",
    "    # A.RandomBrightnessContrast(p=0.5),\n",
    "    # A.GridDistortion(p=0.5, border_mode=cv.BORDER_CONSTANT),\n",
    "    # A.MedianBlur(p=0.5),\n",
    "    # A.RandomToneCurve(p=0.5),\n",
    "    # A.MultiplicativeNoise(p=0.5),\n",
    "    ToTensorV2(),\n",
    "]))\n",
    "example_loader = DataLoader(example_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "example_image, example_mask = next(iter(example_loader))\n",
    "print(f'Image shape: {example_image.shape}')\n",
    "print(f'Mask shape: {example_mask.shape}')\n",
    "\n",
    "unique, counts = np.unique(example_mask, return_counts=True)\n",
    "print(f'Unique values and their counts in mask: {dict(zip(unique, counts))}')\n",
    "\n",
    "# Plot example augmented images and masks\n",
    "fig, ax = plt.subplots(3, 6, figsize=(12, 6))\n",
    "ax = ax.ravel()\n",
    "for i in range(0, 3 * 6, 2):\n",
    "    batch = next(iter(example_loader))\n",
    "    images, masks = batch\n",
    "    image, mask = images[0], masks[0]\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    mask = mask.numpy()\n",
    "    ax[i].imshow(image)\n",
    "    ax[i].axis('off')\n",
    "    ax[i + 1].imshow(mask, cmap='gray')\n",
    "    ax[i + 1].axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=1.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "train_loader, val_loader, test_loader = load_origa(\n",
    "    IMAGE_DIR, MASK_DIR, 0.7, 0.15, 0.15,\n",
    "    train_transform, val_transform, val_transform, BATCH_SIZE, PIN_MEMORY, NUM_WORKERS\n",
    ")\n",
    "\n",
    "\n",
    "def arctan(x):\n",
    "    return 1e-7 + (1 - 2 * 1e-7) * (0.5 + torch.arctan(x) / torch.tensor(np.pi))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize model, loss, optimizer, scheduler, scaler, ...\n",
    "model = Unet(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "# model = UnetPlusPlus(in_channels, out_channels, LAYERS, deep_supervision=DEEP_SUPERVISION).to(DEVICE)\n",
    "# model = Unet3Plus(in_channels, out_channels, LAYERS, deep_supervision=DEEP_SUPERVISION).to(DEVICE)\n",
    "\n",
    "# model = AttentionUnet(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "# model = InceptionUnet(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "\n",
    "# model = ResUnet(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "# model = RUnet(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "# model = R2Unet(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "\n",
    "# model = SqueezeUnet(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "\n",
    "# model = R2AttentionUnet(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "# model = R2UnetPlusPlus(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "\n",
    "# model = ResAttentionUnetPlusPlus(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "# model = RefUnet3PlusCBAM(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "# model = DualRAUnetPlusPlus(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "# model = DualRefUnet3PlusCBAM(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = DiceLoss(out_channels)\n",
    "# criterion = GeneralizedDice(out_channels)\n",
    "# criterion = IoULoss(out_channels)\n",
    "# criterion = FocalLoss(out_channels)\n",
    "# criterion = TverskyLoss(out_channels)\n",
    "# criterion = FocalTverskyLoss(out_channels)\n",
    "# criterion = HausdorffLoss(out_channels)\n",
    "# criterion = BoundaryLoss(out_channels)\n",
    "# criterion = CrossEntropyLoss(out_channels)\n",
    "# criterion = SensitivitySpecificityLoss(out_channels)\n",
    "# criterion = EdgeLoss(out_channels)\n",
    "# criterion = ComboLoss(out_channels)\n",
    "\n",
    "# scheduler = None\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "scaler = None\n",
    "# scaler = torch.cuda.amp.GradScaler()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hist = train_multiclass(\n",
    "#     model, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "#     save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "#     log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "#     save_best_model=False, plot_examples='none',\n",
    "# )\n",
    "\n",
    "hist = train_binary(\n",
    "    model, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "    save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "    save_best_model=False, plot_examples='none',\n",
    ")\n",
    "\n",
    "# model = Unet(3, 1, LAYERS).to(DEVICE)\n",
    "# checkpoint = torch.load(CHECKPOINT_DIR + 'binary-model.pth')\n",
    "# model.load_state_dict(checkpoint)\n",
    "# model2 = RefUnet3PlusCBAM(3, 1, LAYERS, model).to(DEVICE)\n",
    "# optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)\n",
    "# hist = train_cascade(\n",
    "#     model, model2, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "#     save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "#     log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "#     save_best_model=False, plot_examples='none',\n",
    "# )\n",
    "\n",
    "# model = DualRefUnet3PlusCBAM(3, 1, LAYERS).to(DEVICE)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# hist = train_dual(\n",
    "#     model, criterion, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "#     save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "#     log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "#     save_best_model=False, plot_examples='none',\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "used_metrics = sorted([m[6:] for m in hist.keys() if m.startswith('train_')])\n",
    "fig, ax = plt.subplots(4, 4, figsize=(14, 8))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, metric in enumerate(used_metrics):\n",
    "    ax[i].plot(hist[f'train_{metric}'], label=f'train')\n",
    "    ax[i].plot(hist[f'val_{metric}'], label=f'val')\n",
    "    ax[i].set_title(metric[0].upper() + metric[1:].replace('_', ' '))\n",
    "    if metric != 'loss':\n",
    "        ax[i].set_ylim(top=1)\n",
    "    ax[i].legend()\n",
    "\n",
    "for ax in ax[len(used_metrics):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = evaluate(model, criterion, DEVICE, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results_from_loader(test_loader, model, DEVICE, types='all', n_samples=4, save_path=f'{LOGS_DIR}/evaluation.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Work in progress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), CHECKPOINT_DIR + 'model.pth')\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_DIR + 'model.pth')\n",
    "model = Unet(in_channels, out_channels, LAYERS).to(DEVICE)\n",
    "model.load_state_dict(checkpoint)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "\n",
    "model = Unet(in_channels=3, out_channels=num_classes).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss = DiceLoss(num_classes=num_classes)\n",
    "# loss = GeneralizedDice(num_classes=num_classes)\n",
    "# loss = IoULoss(num_classes=num_classes)\n",
    "# loss = FocalLoss(num_classes=num_classes)\n",
    "# loss = TverskyLoss(num_classes=num_classes)\n",
    "# loss = FocalTverskyLoss(num_classes=num_classes)\n",
    "# loss = HausdorffLoss(num_classes=num_classes)\n",
    "# loss = BoundaryLoss(num_classes=num_classes)\n",
    "# loss = CrossEntropy(num_classes=num_classes)\n",
    "# loss = SensitivitySpecificityLoss(num_classes=num_classes)\n",
    "# loss = EdgeLoss(num_classes=num_classes)\n",
    "# loss = ComboLoss(num_classes=num_classes)\n",
    "\n",
    "for epoch in range(5):\n",
    "    acc_loss = 0\n",
    "    for images, masks in val_loader:\n",
    "        images = images.float().to(DEVICE)\n",
    "        masks = masks.long().to(DEVICE)\n",
    "\n",
    "        if num_classes == 1:\n",
    "            masks = torch.where(masks > 0, torch.tensor(1).to(DEVICE), torch.tensor(0).to(DEVICE))\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss_value = loss(outputs, masks)\n",
    "        acc_loss += loss_value.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # plot example\n",
    "    images = images.cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "    if num_classes > 1:\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    else:\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).float().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    _, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(images[0].transpose(1, 2, 0) / 255.0)\n",
    "    ax[1].imshow(masks[0])\n",
    "    ax[2].imshow(preds[0])\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} loss:', acc_loss / len(val_loader))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TODO: find out wtf is going on with the model's output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, masks = next(iter(val_loader))\n",
    "\n",
    "# change order of images\n",
    "# new_order = [0, 1, 0, 3]\n",
    "# images = images[new_order]\n",
    "# masks = masks[new_order]\n",
    "\n",
    "images = images.float().to(DEVICE)\n",
    "masks = masks.long().to(DEVICE)\n",
    "\n",
    "outputs = model(images)\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "# plot probs\n",
    "_, ax = plt.subplots(4, 6, figsize=(18, 12))\n",
    "for i in range(4):\n",
    "    ax[i, 0].imshow(images[i].cpu().numpy().transpose(1, 2, 0) / 255.0)\n",
    "    ax[i, 1].imshow(masks[i].cpu().numpy())\n",
    "    ax[i, 2].imshow(probs[i, 0].cpu().detach().numpy())\n",
    "    ax[i, 3].imshow(probs[i, 1].cpu().detach().numpy())\n",
    "    ax[i, 4].imshow(probs[i, 2].cpu().detach().numpy())\n",
    "    ax[i, 5].imshow(preds[i].cpu().detach().numpy())\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = images[1]\n",
    "mask = masks[1]\n",
    "rotated = torch.cat(\n",
    "    [torch.rot90(image, k=k, dims=(1, 2)).unsqueeze(0) for k in range(4)] + \\\n",
    "    [torch.rot90(torch.flip(image, dims=(1,)), k=k, dims=(1, 2)).unsqueeze(0) for k in range(4)], dim=0)\n",
    "rotated = rotated.to(DEVICE)\n",
    "\n",
    "rotated_mask = torch.cat(\n",
    "    [torch.rot90(mask, k=k, dims=(0, 1)).unsqueeze(0) for k in range(4)] + \\\n",
    "    [torch.rot90(torch.flip(mask, dims=(0,)), k=k, dims=(0, 1)).unsqueeze(0) for k in range(4)], dim=0)\n",
    "\n",
    "outputs = model(rotated)\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "# plot probs\n",
    "fig, ax = plt.subplots(4, 6, figsize=(18, 12))\n",
    "for i in range(4):\n",
    "    ax[i, 0].imshow(rotated[i].cpu().numpy().transpose(1, 2, 0) / 255.0)\n",
    "    ax[i, 1].imshow(rotated_mask[i].cpu().numpy())\n",
    "    ax[i, 2].imshow(probs[i, 0].cpu().detach().numpy())\n",
    "    ax[i, 3].imshow(probs[i, 1].cpu().detach().numpy())\n",
    "    ax[i, 4].imshow(probs[i, 2].cpu().detach().numpy())\n",
    "    ax[i, 5].imshow(preds[i].cpu().detach().numpy())\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = model(image.unsqueeze(0).to(DEVICE))\n",
    "probs = F.softmax(output, dim=1)\n",
    "pred = torch.argmax(probs, dim=1)\n",
    "\n",
    "# plot probs\n",
    "fig, ax = plt.subplots(1, 6, figsize=(18, 3))\n",
    "ax[0].imshow(image.cpu().numpy().transpose(1, 2, 0) / 255.0)\n",
    "ax[1].imshow(mask.cpu().numpy())\n",
    "ax[2].imshow(probs[0, 0].cpu().detach().numpy())\n",
    "ax[3].imshow(probs[0, 1].cpu().detach().numpy())\n",
    "ax[4].imshow(probs[0, 2].cpu().detach().numpy())\n",
    "ax[5].imshow(pred[0].cpu().detach().numpy())\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Contour detection method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from skimage import segmentation\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "images, masks = next(iter(val_loader))\n",
    "images = images.float().to(DEVICE)\n",
    "masks = masks.long().to(DEVICE)\n",
    "\n",
    "image = images[0].cpu().numpy().transpose(1, 2, 0) / 255.0\n",
    "mask = masks[0].cpu().numpy()\n",
    "prediction = np.zeros_like(mask)\n",
    "prediction[16:112, 16:112] = 1\n",
    "prediction[32:96, 32:96] = 2\n",
    "\n",
    "boundaries = segmentation.find_boundaries(mask, mode='inner').astype(np.uint8)\n",
    "marked = image.copy()\n",
    "marked[boundaries == 1] = [0, 0, 0]\n",
    "dist_map = distance_transform_edt(1 - boundaries)\n",
    "dist_map = dist_map / dist_map.max()\n",
    "\n",
    "_, ax = plt.subplots(2, 4, figsize=(15, 7))\n",
    "ax[0, 0].imshow(mask)\n",
    "ax[0, 1].imshow(boundaries)\n",
    "ax[0, 2].imshow(dist_map)\n",
    "ax[0, 3].imshow(dist_map.max() - dist_map)\n",
    "ax[1, 0].imshow(prediction)\n",
    "ax[1, 1].imshow(marked)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Unet(in_channels=3, out_channels=1).to(DEVICE)\n",
    "loss = DiceLoss(num_classes=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(5):\n",
    "    acc_loss = 0\n",
    "    for images, masks in val_loader:\n",
    "\n",
    "        edge_masks = np.zeros((masks.shape[0], masks.shape[1], masks.shape[2]))\n",
    "        for b in range(masks.shape[0]):\n",
    "            mask = masks[b].cpu().numpy()\n",
    "            boundaries = segmentation.find_boundaries(mask, mode='thick').astype(np.uint8)\n",
    "            edge_masks[b] = boundaries\n",
    "        masks = torch.from_numpy(edge_masks)\n",
    "\n",
    "        images = images.float().to(DEVICE)\n",
    "        masks = masks.long().to(DEVICE)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss_value = loss(outputs, masks)\n",
    "        acc_loss += loss_value.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # plot example\n",
    "    images = images.cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "    # probs = F.softmax(outputs, dim=1)\n",
    "    # preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    probs = torch.sigmoid(outputs)\n",
    "    preds = (probs > 0.5).float().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(images[0].transpose(1, 2, 0) / 255.0)\n",
    "    ax[1].imshow(masks[0])\n",
    "    ax[2].imshow(preds[0])\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} loss:', acc_loss / len(val_loader))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
