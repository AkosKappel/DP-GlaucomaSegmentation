{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Glaucoma Segmentation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2 as cv\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from networks import *\n",
    "from training import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMAGE_DIR = '../data/ORIGA/Images_CenterNet_Cropped'\n",
    "MASK_DIR = '../data/ORIGA/Masks_CenterNet_Cropped'\n",
    "LOGS_DIR = '../logs/'\n",
    "CHECKPOINT_DIR = '../checkpoints/'\n",
    "\n",
    "NETWORK_NAME = 'refunet3+cbam'  # unet, raunet++, refunet3+cbam, swinunet\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS_FUNCTION = 'combo'\n",
    "ARCHITECTURE = 'binary'  # multiclass, multilabel, binary, dual, cascade\n",
    "BINARY_TARGET_CLASSES = [1, 2]\n",
    "SCHEDULER = 'plateau'\n",
    "SCALER = 'none'\n",
    "DATASET = 'ORIGA'\n",
    "BASE_CASCADE_MODEL = ''\n",
    "\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 128, 128\n",
    "IN_CHANNELS, OUT_CHANNELS = 3, 1\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "LAYERS = [16, 32, 48, 64, 80]\n",
    "CLASS_WEIGHTS = None\n",
    "SET_SIZES = [0.8, 0.1, 0.1]\n",
    "DROPOUT_2D = 0.2\n",
    "EARLY_STOPPING_PATIENCE = 11\n",
    "LOG_INTERVAL = 5\n",
    "SAVE_INTERVAL = 10\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "USE_WANDB = False\n",
    "POLAR_TRANSFORM = True\n",
    "MULTI_SCALE_INPUT = False\n",
    "DEEP_SUPERVISION = False\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PIN_MEMORY = True if DEVICE == 'cuda' else False\n",
    "\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f'''CONFIGURATION:\n",
    "    PyTorch version: {torch.__version__}\n",
    "    NumPy version: {np.__version__}\n",
    "    OpenCV version: {cv.__version__}\n",
    "\n",
    "    Network: {NETWORK_NAME}\n",
    "    Architecture: {ARCHITECTURE}\n",
    "    Optimizer: {OPTIMIZER}\n",
    "    Loss function: {LOSS_FUNCTION}\n",
    "    Scheduler: {SCHEDULER}\n",
    "    Scaler: {SCALER}\n",
    "    Using device: {DEVICE}\n",
    "\n",
    "    Dataset: {DATASET}\n",
    "    Dataset proportions: {SET_SIZES}\n",
    "    Image directory: {IMAGE_DIR}\n",
    "    Mask directory: {MASK_DIR}\n",
    "    Input image height & width: {IMAGE_HEIGHT}x{IMAGE_WIDTH}\n",
    "    Number of input channels: {IN_CHANNELS}\n",
    "    Number of output channels: {OUT_CHANNELS}\n",
    "\n",
    "    Layers: {LAYERS}\n",
    "    Batch size: {BATCH_SIZE}\n",
    "    Learning rate: {LEARNING_RATE}\n",
    "    Epochs: {EPOCHS}\n",
    "    Class weights: {CLASS_WEIGHTS}\n",
    "    Dropout: {DROPOUT_2D}\n",
    "    Early stopping patience: {EARLY_STOPPING_PATIENCE}\n",
    "\n",
    "    Save interval: {SAVE_INTERVAL}\n",
    "    Log interval: {LOG_INTERVAL}\n",
    "    Number of workers: {NUM_WORKERS}\n",
    "    Pin memory: {PIN_MEMORY}\n",
    "\n",
    "    Weight & Biases: {USE_WANDB}\n",
    "    Polar transform: {POLAR_TRANSFORM}\n",
    "    Multi-scale input: {MULTI_SCALE_INPUT}\n",
    "    Deep supervision: {DEEP_SUPERVISION}''')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polar_transform_partial = partial(polar_transform, radius_ratio=1.0)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, interpolation=cv.INTER_AREA),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=1.0),\n",
    "    A.CLAHE(p=1.0, clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    # A.GridDistortion(p=0.5, border_mode=cv.BORDER_CONSTANT),\n",
    "    # A.MedianBlur(p=0.5),\n",
    "    # A.RandomToneCurve(p=0.5),\n",
    "    # A.MultiplicativeNoise(p=0.5),\n",
    "    # A.Lambda(image=sharpen, p=1.0),\n",
    "    A.Lambda(image=polar_transform_partial, mask=polar_transform_partial) if POLAR_TRANSFORM else A.Lambda(),\n",
    "    # A.Lambda(image=keep_gray_channel),\n",
    "    # A.Lambda(image=keep_red_channel),\n",
    "    # A.Lambda(image=keep_green_channel),\n",
    "    # A.Lambda(image=keep_blue_channel),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, interpolation=cv.INTER_AREA),\n",
    "    A.CLAHE(p=1.0, clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True),\n",
    "    A.Lambda(image=polar_transform_partial, mask=polar_transform_partial) if POLAR_TRANSFORM else A.Lambda(),\n",
    "    # A.Lambda(image=keep_gray_channel),\n",
    "    # A.Lambda(image=keep_red_channel),\n",
    "    # A.Lambda(image=keep_green_channel),\n",
    "    # A.Lambda(image=keep_blue_channel),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "train_loader, val_loader, test_loader = load_dataset(\n",
    "    IMAGE_DIR, MASK_DIR, None, None, *SET_SIZES,\n",
    "    train_transform, val_transform, val_transform, BATCH_SIZE, PIN_MEMORY, NUM_WORKERS,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, masks = next(iter(train_loader))\n",
    "for image, mask in zip(images, masks):\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    mask = mask.numpy()\n",
    "    _, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    ax[0].imshow(image)\n",
    "    ax[1].imshow(mask)\n",
    "    plt.show()\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = None\n",
    "binary_model = None\n",
    "hist = None\n",
    "\n",
    "if NETWORK_NAME == 'unet':\n",
    "    model = Unet(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=LAYERS,\n",
    "        multi_scale_input=MULTI_SCALE_INPUT,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'raunet++':\n",
    "    model = RAUnetPlusPlus(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=LAYERS,\n",
    "        multi_scale_input=MULTI_SCALE_INPUT, deep_supervision=DEEP_SUPERVISION, dropout=DROPOUT_2D,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'refunet3+cbam':\n",
    "    model = RefUnet3PlusCBAM(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=LAYERS,\n",
    "        multi_scale_input=MULTI_SCALE_INPUT, dropout=DROPOUT_2D,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'swinunet':\n",
    "    model = SwinUnet(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, img_size=224, patch_size=4,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'dual-unet':\n",
    "    model = DualUnet(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=LAYERS,\n",
    "        multi_scale_input=MULTI_SCALE_INPUT,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'dual-raunet++':\n",
    "    model = DualRAUnetPlusPlus(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=LAYERS,\n",
    "        multi_scale_input=MULTI_SCALE_INPUT, deep_supervision=DEEP_SUPERVISION, dropout=DROPOUT_2D,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'dual-refunet3+cbam':\n",
    "    model = DualRefUnet3PlusCBAM(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=LAYERS,\n",
    "        multi_scale_input=MULTI_SCALE_INPUT, dropout=DROPOUT_2D,\n",
    "    )\n",
    "\n",
    "if NETWORK_NAME == 'dual-swinunet':\n",
    "    model = DualSwinUnet(\n",
    "        in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, img_size=224, patch_size=4,\n",
    "    )\n",
    "\n",
    "assert model is not None, 'Invalid network name'\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "init_model_weights(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = ComboLoss(num_classes=OUT_CHANNELS if ARCHITECTURE == 'multiclass' else 1, class_weights=CLASS_WEIGHTS)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, verbose=True)\n",
    "scaler = None\n",
    "\n",
    "if BASE_CASCADE_MODEL:\n",
    "    checkpoint = load_checkpoint(BASE_CASCADE_MODEL)\n",
    "    binary_model = checkpoint['model']\n",
    "    print('Loaded base model')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WEIGHT_DECAY = 1e-4\n",
    "# \n",
    "# model = SwinUnet(in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, img_size=224, patch_size=4).to(DEVICE)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#     optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hist = train_multiclass(\n",
    "#     model, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler,\n",
    "#     save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "#     log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "#     save_best_model=True, plot_examples='all', show_plots=True,\n",
    "# )\n",
    "# plot_history(hist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if ARCHITECTURE == 'multiclass':\n",
    "    hist = train_multiclass(\n",
    "        model, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "        save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "        save_best_model=True, plot_examples='none', show_plots=False,\n",
    "        inverse_transform=undo_polar_transform if POLAR_TRANSFORM else None,\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'multilabel':\n",
    "    hist = train_multilabel(\n",
    "        model, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "        save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "        save_best_model=True, plot_examples='none', show_plots=False,\n",
    "        inverse_transform=undo_polar_transform if POLAR_TRANSFORM else None,\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'binary':\n",
    "    hist = train_binary(\n",
    "        model, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "        save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "        save_best_model=True, plot_examples='none', show_plots=False, target_ids=BINARY_TARGET_CLASSES,\n",
    "        inverse_transform=undo_polar_transform if POLAR_TRANSFORM else None,\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'cascade':\n",
    "    assert binary_model is not None, 'Base model not specified'\n",
    "    hist = train_cascade(\n",
    "        binary_model, model, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "        save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "        save_best_model=True, plot_examples='none', show_plots=False,\n",
    "        inverse_transform=undo_polar_transform if POLAR_TRANSFORM else None,\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'dual':\n",
    "    hist = train_dual(\n",
    "        model, criterion, criterion, optimizer, EPOCHS, DEVICE, train_loader, val_loader, scheduler, scaler,\n",
    "        save_interval=SAVE_INTERVAL, early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        log_to_wandb=USE_WANDB, log_dir=LOGS_DIR, log_interval=LOG_INTERVAL, checkpoint_dir=CHECKPOINT_DIR,\n",
    "        save_best_model=True, plot_examples='none', show_plots=False,\n",
    "        inverse_transform=undo_polar_transform if POLAR_TRANSFORM else None,\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_history(hist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if ARCHITECTURE == 'multiclass':\n",
    "    results = evaluate('multiclass', model, test_loader, criterion, DEVICE)\n",
    "\n",
    "if ARCHITECTURE == 'multilabel':\n",
    "    results = evaluate('multilabel', model, test_loader, criterion, DEVICE)\n",
    "\n",
    "if ARCHITECTURE == 'binary':\n",
    "    results = evaluate('binary', model, test_loader, criterion, DEVICE, class_ids=BINARY_TARGET_CLASSES)\n",
    "\n",
    "if ARCHITECTURE == 'dual':\n",
    "    results = evaluate('dual', model, test_loader, criterion, DEVICE)\n",
    "\n",
    "if ARCHITECTURE == 'cascade':\n",
    "    results = evaluate('cascade', model, test_loader, criterion, DEVICE, model0=binary_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if ARCHITECTURE == 'multiclass':\n",
    "    plot_results_from_loader(\n",
    "        'multiclass', test_loader, model, DEVICE, n_samples=4,\n",
    "        save_path=f'{LOGS_DIR}/evaluation.png',\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'multilabel':\n",
    "    plot_results_from_loader(\n",
    "        'multilabel', test_loader, model, DEVICE, n_samples=4,\n",
    "        save_path=f'{LOGS_DIR}/evaluation.png',\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'binary':\n",
    "    plot_results_from_loader(\n",
    "        'binary', test_loader, model, DEVICE, n_samples=4,\n",
    "        save_path=f'{LOGS_DIR}/evaluation.png', class_ids=BINARY_TARGET_CLASSES,\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'dual':\n",
    "    plot_results_from_loader(\n",
    "        'dual', test_loader, model, DEVICE,\n",
    "        n_samples=4, save_path=f'{LOGS_DIR}/evaluation.png',\n",
    "    )\n",
    "\n",
    "if ARCHITECTURE == 'cascade':\n",
    "    plot_results_from_loader(\n",
    "        'cascade', test_loader, model, DEVICE, n_samples=4,\n",
    "        save_path=f'{LOGS_DIR}/evaluation.png', model0=binary_model,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apply CRF post-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, masks = next(iter(test_loader))\n",
    "images = images.float().to(DEVICE)\n",
    "masks = masks.long().to(DEVICE)\n",
    "\n",
    "outputs = model(images)\n",
    "probs = torch.softmax(outputs, dim=1)\n",
    "preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "images = images.detach().cpu().numpy().transpose(0, 2, 3, 1) / 255\n",
    "masks = masks.detach().cpu().numpy()\n",
    "preds = preds.detach().cpu().numpy()\n",
    "probs = probs.detach().cpu().numpy()\n",
    "\n",
    "idx = 0\n",
    "image, mask, pred, prob = images[idx], masks[idx], preds[idx], probs[idx]\n",
    "\n",
    "print(f'{image.shape = }')\n",
    "print(f'{mask.shape = }')\n",
    "print(f'{pred.shape = }')\n",
    "print(f'{prob.shape = }')\n",
    "\n",
    "pred_crf = dense_crf(image, prob)\n",
    "\n",
    "m1 = get_metrics(mask, pred, [[1, 2], [2]])\n",
    "m2 = get_metrics(mask, pred_crf, [[1, 2], [2]])\n",
    "\n",
    "print('Before CRF:', m1['dice_OC'], m1['dice_OD'])\n",
    "print('After CRF:', m2['dice_OC'], m2['dice_OD'])\n",
    "print('Improvement:', m2['dice_OC'] - m1['dice_OC'], m2['dice_OD'] - m1['dice_OD'])\n",
    "\n",
    "_, ax = plt.subplots(1, 4, figsize=(12, 4))\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(mask)\n",
    "ax[2].imshow(pred)\n",
    "ax[3].imshow(pred_crf)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Unet(3, 3, LAYERS).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = DiceLoss(1)\n",
    "\n",
    "trainer = MultilabelTrainer(model, criterion, optimizer, DEVICE)\n",
    "\n",
    "trainer.train_one_epoch(train_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Work in progress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, masks = next(iter(test_loader))\n",
    "images = images.float().to(DEVICE)\n",
    "masks = masks.long().to(DEVICE)\n",
    "\n",
    "outputs = model(images)\n",
    "probs = torch.softmax(outputs, dim=1)\n",
    "preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "met1 = get_metrics(masks, preds, [[1, 2], [2]])\n",
    "\n",
    "images, masks, preds = undo_polar_transform(images, masks, preds)\n",
    "\n",
    "met2 = get_metrics(masks, preds, [[1, 2], [2]])\n",
    "\n",
    "images = images.detach().cpu().numpy().transpose(0, 2, 3, 1) / 255\n",
    "masks = masks.detach().cpu().numpy()\n",
    "preds = preds.detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(8, 12))\n",
    "ax = ax.ravel()\n",
    "for i in range(4):\n",
    "    ax[3 * i].imshow(images[i])\n",
    "    ax[3 * i + 1].imshow(masks[i])\n",
    "    ax[3 * i + 2].imshow(preds[i])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "{k: (met1[k], met2[k]) for k in met1.keys()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), CHECKPOINT_DIR + 'model.pth')\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_DIR + 'multiclass-unet-model.pth')\n",
    "model = Unet(3, 3, LAYERS).to(DEVICE)\n",
    "# model = DualUnet(3, 1, LAYERS).to(DEVICE)\n",
    "criterion = DiceLoss(3)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# binary_model = Unet(3, 1, LAYERS).to(DEVICE)\n",
    "# checkpoint = torch.load(CHECKPOINT_DIR + 'binary-model.pth')\n",
    "# binary_model.load_state_dict(checkpoint)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Non-ellipse: 5, 12\n",
    "# Holes: 0, 20\n",
    "\n",
    "target_batch = 12\n",
    "\n",
    "for batch_idx, (images, masks) in enumerate(test_loader):\n",
    "    if batch_idx != target_batch:\n",
    "        continue\n",
    "    images = images.float().to(DEVICE)\n",
    "    masks = masks.long().to(DEVICE)\n",
    "\n",
    "    outputs = model(images)\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "    met = get_metrics(masks, preds, [[1, 2], [2]])\n",
    "\n",
    "    images = images.detach().cpu().numpy().transpose(0, 2, 3, 1) / 255\n",
    "    masks = masks.detach().cpu().numpy()\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "    print(f'Batch {batch_idx} metrics: {met}')\n",
    "\n",
    "    # Plot results\n",
    "    fig, ax = plt.subplots(4, 3, figsize=(8, 12))\n",
    "    ax = ax.ravel()\n",
    "    for i in range(4):\n",
    "        ax[3 * i].imshow(images[i])\n",
    "        ax[3 * i + 1].imshow(masks[i])\n",
    "        ax[3 * i + 2].imshow(preds[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "image = images[idx]\n",
    "mask = masks[idx]\n",
    "pred = preds[idx]\n",
    "\n",
    "mask_od, mask_oc = separate_disc_and_cup_mask(mask)\n",
    "pred_od, pred_oc = separate_disc_and_cup_mask(pred)\n",
    "\n",
    "# plot image, OD mask and OC mask\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax = ax.ravel()\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(pred_od)\n",
    "ax[2].imshow(pred_oc)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "get_metrics(mask_oc, pred_oc, [[1, 2]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask1 = apply_largest_component_selection(pred)\n",
    "mask2 = apply_hole_filling(pred)\n",
    "mask3 = apply_largest_component_selection(apply_hole_filling(pred))\n",
    "mask4 = apply_hole_filling(apply_largest_component_selection(pred))\n",
    "mask5 = apply_ellipse_fitting(pred)\n",
    "\n",
    "# Plot results\n",
    "_, ax = plt.subplots(2, 4, figsize=(12, 7))\n",
    "ax = ax.ravel()\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "ax[0].set_title('Image')\n",
    "ax[0].imshow(image)\n",
    "ax[1].set_title('Ground truth')\n",
    "ax[1].imshow(mask)\n",
    "ax[2].set_title('Prediction')\n",
    "ax[2].imshow(pred)\n",
    "ax[3].set_title('Largest component')\n",
    "ax[3].imshow(mask1)\n",
    "ax[4].set_title('Hole filling')\n",
    "ax[4].imshow(mask2)\n",
    "ax[5].set_title('Largest + hole')\n",
    "ax[5].imshow(mask3)\n",
    "ax[6].set_title('Hole + largest')\n",
    "ax[6].imshow(mask4)\n",
    "ax[7].set_title('Ellipse fitting')\n",
    "ax[7].imshow(mask5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smoothed_od_mask = smooth_contours(pred_od)\n",
    "smoothed_oc_mask = smooth_contours(pred_oc)\n",
    "smoothed_mask = smoothed_od_mask + smoothed_oc_mask\n",
    "\n",
    "# Plot results\n",
    "_, ax = plt.subplots(1, 4, figsize=(12, 5))\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(pred)\n",
    "ax[2].imshow(smoothed_od_mask)\n",
    "ax[3].imshow(smoothed_oc_mask)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def on_trackbar(s):\n",
    "    s = s / 10\n",
    "    smoothed_mask = smooth_contours(pred_oc, s)\n",
    "\n",
    "    m = get_metrics(mask_oc, smoothed_mask, [[1, 2]])\n",
    "    print(m['dice_OD'])\n",
    "\n",
    "    # Display the original and smoothed masks side by side\n",
    "    side_by_side = np.hstack((mask_oc * 255, pred_oc * 255, smoothed_mask * 255))\n",
    "    cv.imshow('Contour Smoothing', side_by_side)\n",
    "\n",
    "\n",
    "cv.namedWindow('Contour Smoothing')\n",
    "cv.createTrackbar('S Parameter', 'Contour Smoothing', 5, 100, on_trackbar)\n",
    "initial_s = 1.0\n",
    "on_trackbar(initial_s)\n",
    "\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "snake_img = snakes(pred_oc)\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 5))\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(mask_oc)\n",
    "ax[2].imshow(pred_oc)\n",
    "ax[3].imshow(snake_img)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def on_trackbar(a, b, c):\n",
    "    a = a / 10\n",
    "    b = b / 10\n",
    "    c = c / 10\n",
    "\n",
    "    snake_mask = snakes(pred_oc, a, b, c)\n",
    "\n",
    "    m = get_metrics(mask_oc, snake_mask, [[1, 2]])\n",
    "    print(m['dice_OD'])\n",
    "\n",
    "    # Display the original and smoothed masks side by side\n",
    "    side_by_side = np.hstack((mask_oc * 255, pred_oc * 255, snake_mask * 255))\n",
    "    cv.imshow('Snake', side_by_side)\n",
    "\n",
    "\n",
    "def on_trackbar1(a):\n",
    "    global initial_alpha\n",
    "    initial_alpha = a\n",
    "    on_trackbar(initial_alpha, initial_beta, initial_gamma)\n",
    "\n",
    "\n",
    "def on_trackbar2(b):\n",
    "    global initial_beta\n",
    "    initial_beta = b\n",
    "    on_trackbar(initial_alpha, initial_beta, initial_gamma)\n",
    "\n",
    "\n",
    "def on_trackbar3(c):\n",
    "    global initial_gamma\n",
    "    initial_gamma = c\n",
    "    on_trackbar(initial_alpha, initial_beta, initial_gamma)\n",
    "\n",
    "\n",
    "initial_alpha = 0.1\n",
    "initial_beta = 2.0\n",
    "initial_gamma = 5.0\n",
    "\n",
    "cv.namedWindow('Snake')\n",
    "cv.createTrackbar('alpha', 'Snake', 1, 50, on_trackbar1)\n",
    "cv.createTrackbar('beta', 'Snake', 20, 50, on_trackbar2)\n",
    "cv.createTrackbar('gamma', 'Snake', 50, 100, on_trackbar3)\n",
    "on_trackbar(initial_alpha, initial_beta, initial_gamma)\n",
    "\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Contour detection method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from skimage import segmentation\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "images, masks = next(iter(val_loader))\n",
    "images = images.float().to(DEVICE)\n",
    "masks = masks.long().to(DEVICE)\n",
    "\n",
    "image = images[0].cpu().numpy().transpose(1, 2, 0) / 255.0\n",
    "mask = masks[0].cpu().numpy()\n",
    "prediction = np.zeros_like(mask)\n",
    "prediction[16:112, 16:112] = 1\n",
    "prediction[32:96, 32:96] = 2\n",
    "\n",
    "boundaries = segmentation.find_boundaries(mask, mode='inner').astype(np.uint8)\n",
    "marked = image.copy()\n",
    "marked[boundaries == 1] = [0, 0, 0]\n",
    "dist_map = distance_transform_edt(1 - boundaries)\n",
    "dist_map = dist_map / dist_map.max()\n",
    "\n",
    "_, ax = plt.subplots(2, 4, figsize=(15, 7))\n",
    "ax[0, 0].imshow(mask)\n",
    "ax[0, 1].imshow(boundaries)\n",
    "ax[0, 2].imshow(dist_map)\n",
    "ax[0, 3].imshow(dist_map.max() - dist_map)\n",
    "ax[1, 0].imshow(prediction)\n",
    "ax[1, 1].imshow(marked)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Unet(in_channels=3, out_channels=1).to(DEVICE)\n",
    "loss = DiceLoss(num_classes=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(5):\n",
    "    acc_loss = 0\n",
    "    for images, masks in val_loader:\n",
    "\n",
    "        edge_masks = np.zeros((masks.shape[0], masks.shape[1], masks.shape[2]))\n",
    "        for b in range(masks.shape[0]):\n",
    "            mask = masks[b].cpu().numpy()\n",
    "            boundaries = segmentation.find_boundaries(mask, mode='thick').astype(np.uint8)\n",
    "            edge_masks[b] = boundaries\n",
    "        masks = torch.from_numpy(edge_masks)\n",
    "\n",
    "        images = images.float().to(DEVICE)\n",
    "        masks = masks.long().to(DEVICE)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss_value = loss(outputs, masks)\n",
    "        acc_loss += loss_value.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # plot example\n",
    "    images = images.cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "    # probs = F.softmax(outputs, dim=1)\n",
    "    # preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    probs = torch.sigmoid(outputs)\n",
    "    preds = (probs > 0.5).float().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(images[0].transpose(1, 2, 0) / 255.0)\n",
    "    ax[1].imshow(masks[0])\n",
    "    ax[2].imshow(preds[0])\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} loss:', acc_loss / len(val_loader))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretability"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Unet(in_channels=3, out_channels=3, features=[32, 64, 128, 256, 512]).to(DEVICE)\n",
    "checkpoint = torch.load(CHECKPOINT_DIR + 'best-multiclass-unet-model.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "images, masks = next(iter(val_loader))\n",
    "images = images.float().to(DEVICE)\n",
    "masks = masks.long().to(DEVICE)\n",
    "# print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Guided Backpropagation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GuidedBackpropagation:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.hooks = []\n",
    "        self.hook_layers()\n",
    "\n",
    "    def hook_layers(self):\n",
    "        def relu_hook_function(module, grad_in, grad_out):\n",
    "            if isinstance(module, torch.nn.ReLU):\n",
    "                return (torch.clamp(grad_in[0], min=0.),)\n",
    "\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                hook = module.register_backward_hook(relu_hook_function)\n",
    "                self.hooks.append(hook)\n",
    "\n",
    "    def unhook_layers(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "\n",
    "    def guided_backward(self, image, class_idx=None, to_numpy=True):\n",
    "        inputs = image.clone()\n",
    "        inputs.requires_grad = True\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(outputs, dim=1, keepdim=True)\n",
    "\n",
    "        onehot = torch.zeros_like(outputs)\n",
    "        onehot[0][class_idx] = 1\n",
    "\n",
    "        outputs.backward(gradient=onehot)\n",
    "        gradients = inputs.grad\n",
    "\n",
    "        # Keep only positive gradients\n",
    "        gradients = F.relu(gradients)\n",
    "\n",
    "        # Normalize\n",
    "        min_grad, max_grad = torch.min(gradients), torch.max(gradients)\n",
    "        gradients = (gradients - min_grad) / (max_grad - min_grad)\n",
    "        gradients = gradients.squeeze()\n",
    "\n",
    "        if to_numpy:\n",
    "            # Move channel axis to the last dimension\n",
    "            gradients = gradients.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        return gradients\n",
    "\n",
    "\n",
    "input_image = cv.imread(r\"C:\\Users\\ASUS\\PycharmProjects\\DP-GlaucomaSegmentation\\data\\ORIGA\\Images_Cropped\\049.jpg\")\n",
    "input_image = cv.cvtColor(input_image, cv.COLOR_BGR2RGB)\n",
    "input_image = cv.resize(input_image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "input_image = torch.tensor(input_image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "input_image = input_image.to(DEVICE)\n",
    "\n",
    "guided_bp = GuidedBackpropagation(model)\n",
    "guided_grads = guided_bp.guided_backward(input_image, class_idx=2)\n",
    "\n",
    "print(guided_grads.shape, guided_grads.min(), guided_grads.max())\n",
    "\n",
    "input_image = input_image.cpu().numpy()[0].transpose(1, 2, 0) / 255.0\n",
    "\n",
    "_, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(input_image)\n",
    "ax[1].imshow(guided_grads)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grad-CAM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.modules = dict(model.named_modules())\n",
    "        if isinstance(target_layer, str):\n",
    "            assert target_layer in self.modules.keys(), \\\n",
    "                f'Invalid target layer: {target_layer}, available layers: {self.modules.keys()}'\n",
    "            self.target_layer = self.modules[target_layer]\n",
    "        else:\n",
    "            assert target_layer in self.modules.values(), \\\n",
    "                f'Invalid target layer: {target_layer}, available layers: {self.modules.keys()}'\n",
    "            self.target_layer = target_layer\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hooks = []\n",
    "        self.register_hooks()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, module_input, module_output):\n",
    "            self.activations = module_output\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "\n",
    "        hook = self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.hooks.append(hook)\n",
    "        hook = self.target_layer.register_full_backward_hook(backward_hook)\n",
    "        self.hooks.append(hook)\n",
    "\n",
    "    def unregister_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "\n",
    "    def __call__(self, inputs, class_idx=None, to_numpy=True):\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self.model(inputs)\n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        onehot = torch.zeros(outputs.size(), dtype=torch.float32, device=inputs.device)\n",
    "        onehot[0][class_idx] = 1\n",
    "        outputs.backward(gradient=onehot)\n",
    "\n",
    "        # Global average pooling to obtain the pooled gradients\n",
    "        weights = torch.mean(self.gradients, dim=(0, 2, 3), keepdim=True)\n",
    "        # Weight the channels by corresponding gradients\n",
    "        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)\n",
    "        # Clamp negative values to zero\n",
    "        cam = F.relu(cam)\n",
    "        # Normalize to [0, 1]\n",
    "        cam /= torch.max(cam)\n",
    "        # Remove batch dimension\n",
    "        cam = cam.squeeze()\n",
    "\n",
    "        if to_numpy:\n",
    "            cam = cam.detach().cpu().numpy()\n",
    "\n",
    "        return cam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_layers = dict(model.named_modules())\n",
    "# print(model_layers.keys())\n",
    "target_layer = model_layers['decoder.conv2.conv']\n",
    "\n",
    "image = images[3:4]\n",
    "mask = masks[3:4]\n",
    "\n",
    "outputs = model(image)\n",
    "probs = torch.softmax(outputs, dim=1)\n",
    "preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "heatmap = gradcam(image, class_idx=2)\n",
    "heatmap = cv.resize(heatmap, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "heatmap = np.uint8(255 * heatmap / np.max(heatmap))\n",
    "\n",
    "plt.imshow(heatmap)\n",
    "\n",
    "image = image.squeeze().detach().cpu().numpy().transpose(1, 2, 0) / 255\n",
    "mask = mask.squeeze().detach().cpu().numpy()\n",
    "pred = preds.squeeze().detach().cpu().numpy()\n",
    "\n",
    "overlay = cv.applyColorMap(heatmap, cv.COLORMAP_JET)\n",
    "overlay = cv.cvtColor(overlay, cv.COLOR_BGR2RGB) / 255\n",
    "\n",
    "alpha = 0.5\n",
    "combined = alpha * overlay + (1 - alpha) * image\n",
    "\n",
    "_, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(mask)\n",
    "ax[2].imshow(pred)\n",
    "ax[3].imshow(overlay)\n",
    "ax[4].imshow(combined)\n",
    "plt.show()\n",
    "\n",
    "gradcam.unregister_hooks()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Guided Grad-CAM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GuidedGradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.gradcam = GradCAM(model, target_layer)\n",
    "        self.guided_bp = GuidedBackpropagation(model)\n",
    "\n",
    "    def __call__(self, inputs, class_idx=None):\n",
    "        cam = self.gradcam(inputs, class_idx, to_numpy=True)\n",
    "        grads = self.guided_bp.guided_backward(inputs, class_idx, to_numpy=True)\n",
    "\n",
    "        # Resize cam to guided gradients' shape\n",
    "        cam = cv.resize(cam, (grads.shape[0], grads.shape[1]))\n",
    "        cam = np.expand_dims(cam, axis=2)\n",
    "\n",
    "        return grads * cam\n",
    "\n",
    "\n",
    "model_layers = dict(model.named_modules())\n",
    "target_layer = model_layers['decoder.conv2.conv']\n",
    "\n",
    "image = images[3:4]\n",
    "mask = masks[3:4]\n",
    "\n",
    "outputs = model(image)\n",
    "probs = torch.softmax(outputs, dim=1)\n",
    "preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "guided_gradcam = GuidedGradCAM(model, target_layer)\n",
    "guided_gradcam_mask = guided_gradcam(image, class_idx=2)\n",
    "guided_gradcam_mask = cv.resize(guided_gradcam_mask, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "guided_gradcam_mask = np.uint8(255 * guided_gradcam_mask / np.max(guided_gradcam_mask))\n",
    "\n",
    "_, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(image.squeeze().detach().cpu().numpy().transpose(1, 2, 0) / 255)\n",
    "ax[1].imshow(guided_gradcam_mask)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
