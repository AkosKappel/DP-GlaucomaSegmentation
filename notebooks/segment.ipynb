{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-23T10:49:02.832335Z",
     "start_time": "2024-04-23T10:48:56.754927Z"
    }
   },
   "source": [
    "import albumentations as A\n",
    "import cv2 as cv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from functools import partial\n",
    "\n",
    "from modules import *\n",
    "from networks import *\n",
    "from training import *\n",
    "from ROI import CenterNet, detect_roi, preprocess_centernet_input"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CenterNet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b09848f3486dfb0"
  },
  {
   "cell_type": "code",
   "source": [
    "images_dir = '../data/ImagesForSegmentation'\n",
    "test_images = [os.path.join(images_dir, f) for f in os.listdir(images_dir)]\n",
    "test_images = list(filter(lambda x: os.path.isfile(x), test_images))\n",
    "test_images"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T10:49:36.940475Z",
     "start_time": "2024-04-23T10:49:36.918399Z"
    }
   },
   "id": "1f695a599498e714",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/ImagesForSegmentation\\\\1082715_T_1082715_20151212_111331_Color_L_01.jpg',\n",
       " '../data/ImagesForSegmentation\\\\1085032_T_1085032_20151219_113812_Color_R_01.jpg',\n",
       " '../data/ImagesForSegmentation\\\\1107477_T_1107477_20160225_122700_Color_L_01.jpg',\n",
       " '../data/ImagesForSegmentation\\\\1119879_T_1119879_20160328_111556_Color_R_01.jpg',\n",
       " '../data/ImagesForSegmentation\\\\1120696_T_1120696_20160330_104233_Color_L_01.jpg',\n",
       " '../data/ImagesForSegmentation\\\\1205301_T_1205301_20161027_134554_Color_R_01.jpg',\n",
       " '../data/ImagesForSegmentation\\\\184303_T_184303_20170217_112031_Color_R_01.jpg',\n",
       " '../data/ImagesForSegmentation\\\\28877_T_28877_20151224_095119_Color_R_01.jpg',\n",
       " '../data/ImagesForSegmentation\\\\535502_T_535502_20170124_155432_Color_L_01.jpg',\n",
       " '../data/ImagesForSegmentation\\\\833620_T_833620_20160128_102311_Color_R_01.jpg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "MODEL_PATH = '../models/roi/centernet.pth'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CenterNet(n_classes=1, scale=4, base='resnet18', custom=True)\n",
    "state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "print('Model loaded from', MODEL_PATH)\n",
    "model = model.to(DEVICE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T10:54:42.608015Z",
     "start_time": "2024-04-23T10:54:42.347998Z"
    }
   },
   "id": "8c89ffadb0294721",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/roi/centernet.pth\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": [
    "INPUT_SIZE = 512\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(INPUT_SIZE, INPUT_SIZE, interpolation=cv.INTER_AREA),\n",
    "    A.Normalize(mean=(0.9400, 0.6225, 0.3316), std=(0.1557, 0.1727, 0.1556)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['labels']))\n",
    "\n",
    "dst_dir = os.path.join(images_dir, 'ROI')\n",
    "if not os.path.exists(dst_dir):\n",
    "    os.makedirs(dst_dir)\n",
    "\n",
    "roi_test_images = []\n",
    "for file in test_images:\n",
    "    new_img = preprocess_centernet_input(file)\n",
    "    roi_image, _ = detect_roi(\n",
    "        model, new_img, None, transform, INPUT_SIZE,\n",
    "        device=DEVICE, small_margin=32, roi_size=512,\n",
    "    )\n",
    "    img_path = os.path.join(dst_dir, os.path.basename(file))\n",
    "    cv.imwrite(img_path, cv.cvtColor(roi_image, cv.COLOR_RGB2BGR))\n",
    "    roi_test_images.append(img_path)\n",
    "    # plt.imshow(roi_image)\n",
    "    # plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db989c7c545328c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dual"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3c5f0b11f3bec88"
  },
  {
   "cell_type": "code",
   "source": [
    "images_dir = '../data/ORIGA/ROI/TrainImages'\n",
    "masks_dir = '../data/ORIGA/ROI/TrainMasks'\n",
    "images = [os.path.join(images_dir, f) for f in os.listdir(images_dir)]\n",
    "masks = [os.path.join(masks_dir, f) for f in os.listdir(masks_dir)]\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, interpolation=cv.INTER_AREA),\n",
    "    A.CLAHE(p=1.0, clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "loader = load_dataset(images, masks, val_transform, batch_size=4, shuffle=True, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T09:39:33.282248Z",
     "start_time": "2024-04-23T09:39:33.127258Z"
    }
   },
   "id": "9a98af1425760ba8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 325 samples in 82 batches.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "checkpoint = load_checkpoint('../models/polar/ref/dual.pth', map_location=DEVICE)\n",
    "model = checkpoint['model'].to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T09:44:22.231929Z",
     "start_time": "2024-04-23T09:44:22.057175Z"
    }
   },
   "id": "7f37e6890e4c0e0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint: ../models/polar/ref/dual.pth\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP OC: 207015, FN OC: 185796, FP OD: 261919, FN OD: 127937\n"
     ]
    }
   ],
   "source": [
    "thresh = 0.5\n",
    "\n",
    "fp_oc = 0\n",
    "fn_oc = 0\n",
    "fp_od = 0\n",
    "fn_od = 0\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (images, masks) in enumerate(loader):\n",
    "        images = images.to(DEVICE).float()\n",
    "        masks = masks.to(DEVICE).long()\n",
    "\n",
    "        preds, *_ = predict('dual', model, images, masks)\n",
    "        met = get_metrics(masks, preds, [[1, 2], [2]])\n",
    "\n",
    "        fp_oc += met['fp_OC']\n",
    "        fn_oc += met['fn_OC']\n",
    "        fp_od += met['fp_OD']\n",
    "        fn_od += met['fn_OD']\n",
    "\n",
    "print(f'FP OC: {fp_oc}, FN OC: {fn_oc}, FP OD: {fp_od}, FN OD: {fn_od}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T18:06:42.301938200Z",
     "start_time": "2024-02-04T18:06:23.379082500Z"
    }
   },
   "id": "5cdb8b83050d67a2",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "source": [
    "preds = []\n",
    "save_dir = '../data/ImagesForSegmentation/DualArchitectureSegmentation'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, image in enumerate(roi_test_images):\n",
    "        img = cv.imread(image)\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        input_img = val_transform(image=img)['image']\n",
    "        input_img = input_img.unsqueeze(0).to(DEVICE).float()\n",
    "\n",
    "        pred, *_ = predict('dual', model, input_img, None)\n",
    "        pred = pred.squeeze(0).squeeze(0).cpu().numpy()\n",
    "        preds.append(pred)\n",
    "\n",
    "        pred = fill_holes(pred)\n",
    "        pred = keep_largest_component(pred)\n",
    "\n",
    "        vcdr = calculate_vCDR(pred)\n",
    "\n",
    "        contours = get_contour_image(img, pred)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(contours)\n",
    "        plt.title(f'vCDR: {vcdr:.4f}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_dir}/{os.path.basename(image)}')\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc2309e781fe24c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cascade"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e163a4a617f1c610"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint: ./models/binary-raunet.pth\n",
      "=> Loading checkpoint: ./models/cascade-raunet.pth\n"
     ]
    }
   ],
   "source": [
    "checkpoint = load_checkpoint('./models/binary-raunet.pth')\n",
    "binary_model = checkpoint['model'].to(DEVICE)\n",
    "\n",
    "checkpoint = load_checkpoint('./models/cascade-raunet.pth')\n",
    "model = checkpoint['model'].to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T18:37:59.279974600Z",
     "start_time": "2024-02-04T18:37:58.816184700Z"
    }
   },
   "id": "a715b7df063bbf1d",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP OC: 154049, FN OC: 139173, FP OD: 159846, FN OD: 113973\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "thresh = 0.5\n",
    "\n",
    "fp_oc = 0\n",
    "fn_oc = 0\n",
    "fp_od = 0\n",
    "fn_od = 0\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "binary_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (images, masks) in enumerate(loader):\n",
    "        images = images.to(device).float()\n",
    "        masks = masks.to(device).long()\n",
    "\n",
    "        preds, *_ = predict('cascade', model, images, masks, model0=binary_model)\n",
    "        met = get_metrics(masks, preds, [[1, 2], [2]])\n",
    "\n",
    "        fp_oc += met['fp_OC']\n",
    "        fn_oc += met['fn_OC']\n",
    "        fp_od += met['fp_OD']\n",
    "        fn_od += met['fn_OD']\n",
    "\n",
    "print(f'FP OC: {fp_oc}, FN OC: {fn_oc}, FP OD: {fp_od}, FN OD: {fn_od}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T18:38:52.713152100Z",
     "start_time": "2024-02-04T18:38:29.656498500Z"
    }
   },
   "id": "defe28627a5fd928",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "preds = []\n",
    "save_dir = '../data/ImagesForSegmentation/CascadeArchitectureSegmentation'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "model.eval()\n",
    "binary_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, image in enumerate(roi_test_images):\n",
    "        img = cv.imread(image)\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        input_img = val_transform(image=img)['image']\n",
    "        input_img = input_img.unsqueeze(0).to(device).float()\n",
    "\n",
    "        pred, *_ = predict('cascade', model, input_img, None, model0=binary_model)\n",
    "        pred = pred.squeeze(0).squeeze(0).cpu().numpy()\n",
    "        preds.append(pred)\n",
    "\n",
    "        pred = fill_holes(pred, False)\n",
    "        pred = keep_largest_component(pred, False)\n",
    "\n",
    "        vcdr = calculate_vCDR(pred)\n",
    "\n",
    "        contours = get_contour_image(img, pred, colors=[(0, 0, 0), (0, 0, 255)])\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(contours)\n",
    "        plt.title(f'vCDR: {vcdr:.4f}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_dir}/{os.path.basename(image)}')\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fbc4bc86a1f53ea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "776b2b3b44159e8d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8aed25d0474c2969",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
