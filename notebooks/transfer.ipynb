{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import defaultdict\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from networks import *\n",
    "from training import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMAGE_DIR = '../data/ORIGA/Images_Cropped'\n",
    "MASK_DIR = '../data/ORIGA/Masks_Cropped'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 128, 128\n",
    "BATCH_SIZE = 4\n",
    "PIN_MEMORY = True\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "IN_CHANNELS = 3\n",
    "OUT_CHANNELS = 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb5bc16a352bdf6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=1.0),\n",
    "    # A.Lambda(image=occlude),\n",
    "    # A.Lambda(image=polar_transform, mask=polar_transform),\n",
    "    # A.Lambda(image=keep_gray_channel),\n",
    "    # A.Lambda(image=keep_red_channel),\n",
    "    # A.Lambda(image=keep_green_channel),\n",
    "    # A.Lambda(image=keep_blue_channel),\n",
    "    # A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.255]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    # A.Lambda(image=polar_transform, mask=polar_transform),\n",
    "    # A.Lambda(image=keep_gray_channel),\n",
    "    # A.Lambda(image=keep_red_channel),\n",
    "    # A.Lambda(image=keep_green_channel),\n",
    "    # A.Lambda(image=keep_blue_channel),\n",
    "    # A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.255]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "train_loader, val_loader, test_loader = load_origa(\n",
    "    IMAGE_DIR, MASK_DIR, 0.7, 0.15, 0.15,\n",
    "    train_transform, val_transform, val_transform, BATCH_SIZE, PIN_MEMORY, NUM_WORKERS,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9c8be56aa01e293"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BACKBONE = 'resnet34'\n",
    "LR = 1e-4\n",
    "EPOCHS = 5\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=BACKBONE,\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=IN_CHANNELS,\n",
    "    classes=OUT_CHANNELS,\n",
    "    # decoder_attention_type='scse',\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "preprocess_input = get_preprocessing_fn(BACKBONE, pretrained='imagenet')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8622daf6e5eb764"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Freeze encoder weights\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65ff3cb4bd49417f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hist = defaultdict(list)\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}:')\n",
    "\n",
    "    model.train()\n",
    "    metrics = defaultdict(list)\n",
    "    for images, masks in train_loader:\n",
    "        images = images.float().to(DEVICE)\n",
    "        masks = masks.long().to(DEVICE)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metrics['loss'].append(loss.item())\n",
    "        update_metrics(masks, preds, metrics, [[1, 2], [2]])\n",
    "    update_history(hist, {k: np.mean(v) for k, v in metrics.items()}, prefix='train')\n",
    "    print(f'''Training:\n",
    "    Loss: {hist['train_loss'][-1]:.4f}\n",
    "    Accuracy: {hist['train_accuracy_OD'][-1]:.4f} (Disc), {hist['train_accuracy_OC'][-1]:.4f} (Cup)\n",
    "    Dice: {hist['train_dice_OD'][-1]:.4f} (Disc), {hist['train_dice_OC'][-1]:.4f} (Cup)\n",
    "    ''')\n",
    "\n",
    "    model.eval()\n",
    "    metrics = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.float().to(DEVICE)\n",
    "            masks = masks.long().to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            metrics['loss'].append(loss.item())\n",
    "            update_metrics(masks, preds, metrics, [[1, 2], [2]])\n",
    "        update_history(hist, {k: np.mean(v) for k, v in metrics.items()}, prefix='val')\n",
    "    print(f'''Validation:\n",
    "    Loss: {hist['val_loss'][-1]:.4f}\n",
    "    Accuracy: {hist['val_accuracy_OD'][-1]:.4f} (Disc), {hist['val_accuracy_OC'][-1]:.4f} (Cup)\n",
    "    Dice: {hist['val_dice_OD'][-1]:.4f} (Disc), {hist['val_dice_OC'][-1]:.4f} (Cup)\n",
    "    ''')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92558cf7ee5bf6d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_history(hist)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b891f70ee1f56eee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot some predictions\n",
    "model.eval()\n",
    "images, masks = next(iter(test_loader))\n",
    "images = images.float().to(DEVICE)\n",
    "masks = masks.long().to(DEVICE)\n",
    "\n",
    "outputs = model(images)\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "images = images.detach().cpu().numpy().transpose(0, 2, 3, 1) / 255\n",
    "masks = masks.detach().cpu().numpy()\n",
    "preds = preds.detach().cpu().numpy()\n",
    "\n",
    "_, ax = plt.subplots(3, 3, figsize=(15, 15))\n",
    "for i in range(3):\n",
    "    ax[i, 0].imshow(images[i])\n",
    "    ax[i, 1].imshow(masks[i])\n",
    "    ax[i, 2].imshow(preds[i])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42e75daef7359596"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6db202817880a33e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
