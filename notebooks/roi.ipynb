{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Region of Interest (ROI) Extraction from ORIGA dataset images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b2c93029bc3f04b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Findings so far:\n",
    "- brightest spot & template matching works pretty well\n",
    "- intensity weighted centroid is not very good\n",
    "- thresholding a channel gives too big ROI areas\n",
    "- TM can be improved by using more templates and averaging the results\n",
    "- maybe a combination of TC and TM could be good\n",
    "- TM takes too long (use rescaled smaller image and template)\n",
    "- BS often cuts through the OD (try to find other method for cropping the square around the brightest point)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6692ed681b03db04"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:32:21.875797200Z",
     "start_time": "2023-09-20T11:32:21.657188600Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "base_dir = '../data/ORIGA/Images'\n",
    "base_mask_dir = '../data/ORIGA/Masks'\n",
    "files = os.listdir(base_dir)\n",
    "file = os.path.join(base_dir, files[217])\n",
    "bgr_img = cv.imread(file)\n",
    "rgb_img = cv.cvtColor(bgr_img, cv.COLOR_BGR2RGB)\n",
    "grey_img = cv.cvtColor(bgr_img, cv.COLOR_BGR2GRAY)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T18:14:55.202059400Z",
     "start_time": "2023-09-20T18:14:55.024786200Z"
    }
   },
   "id": "c50284084c9d1ecf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Method 1: Intensity weighted centroid"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdb877e7c32f22bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import circular_kernel, gaussian_kernel\n",
    "\n",
    "\n",
    "def intensity_weighted_centroid(image, width: int = 512, height: int = 512, channel: int = -1,\n",
    "                                equalize: bool = True, clahe: bool = True, square: bool = True,\n",
    "                                dampening: str = 'circular', k_size: int = None, quantile: float = None):\n",
    "    # Select the channel that is going to be used for centroid calculation\n",
    "    weights = image[..., channel] if channel != -1 else cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Equalize histogram to increase contrast\n",
    "    if equalize:\n",
    "        weights = cv.equalizeHist(weights)\n",
    "\n",
    "    # Contrast Limited Adaptive Histogram Equalization\n",
    "    if clahe:\n",
    "        clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(10, 10))\n",
    "        weights = clahe.apply(weights)\n",
    "\n",
    "    # Convert to 32-bit float and normalize\n",
    "    weights = weights.astype(np.float32)\n",
    "    weights -= weights.min()\n",
    "    weights = weights / weights.max()\n",
    "\n",
    "    # Apply Gaussian blur to smooth out the image\n",
    "    if k_size is not None:\n",
    "        weights = cv.GaussianBlur(weights, (k_size, k_size), 0)\n",
    "\n",
    "    # Square the weights to increase the contrast\n",
    "    if square:\n",
    "        weights = weights ** 2\n",
    "        weights = weights / weights.sum()\n",
    "\n",
    "    # Dampen the weights close to the corners\n",
    "    dampening = dampening.lower()\n",
    "    if dampening == 'circular':\n",
    "        damping_map = circular_kernel(weights.shape[1], weights.shape[0])\n",
    "    elif dampening == 'gaussian':\n",
    "        damping_map = gaussian_kernel(weights.shape[1], weights.shape[0])\n",
    "    else:\n",
    "        damping_map = np.ones_like(weights)\n",
    "    weights = weights * damping_map\n",
    "\n",
    "    # Cut off the bottom % of the weights\n",
    "    if quantile is not None:\n",
    "        weights[weights < np.quantile(weights, quantile)] = 0\n",
    "\n",
    "    # Find the centroid\n",
    "    x = np.arange(weights.shape[1])\n",
    "    y = np.arange(weights.shape[0])\n",
    "    x, y = np.meshgrid(x, y)\n",
    "\n",
    "    x_weighted = x * weights\n",
    "    y_weighted = y * weights\n",
    "    total_intensity = np.sum(weights)\n",
    "\n",
    "    x_mean = np.sum(x_weighted) / total_intensity\n",
    "    y_mean = np.sum(y_weighted) / total_intensity\n",
    "\n",
    "    # _, ax = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    # ax = ax.ravel()\n",
    "    # ax[0].imshow(x)\n",
    "    # ax[1].imshow(y)\n",
    "    # ax[2].imshow(weights)\n",
    "    # ax[3].imshow(x_weighted)\n",
    "    # ax[4].imshow(y_weighted)\n",
    "    # ax[5].imshow(image)\n",
    "    # plt.show()\n",
    "\n",
    "    return int(x_mean - width / 2), int(y_mean - height / 2), width, height\n",
    "\n",
    "\n",
    "img = rgb_img.copy()\n",
    "x, y, w, h = intensity_weighted_centroid(\n",
    "    img, 512, 512, channel=-1, equalize=True, clahe=True, square=True,\n",
    "    dampening='circular', k_size=65, quantile=0.9,\n",
    ")\n",
    "cv.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 10)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca30f24fc0e10bce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Method 2: Brightest spot algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1127a8dd84e111cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def brightest_spot(image, width: int = 512, height: int = 512, channel: int = -1,\n",
    "                   dampening: str = 'circular', k_size: int = 65):\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2GRAY) if channel == -1 else image[..., channel]\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "\n",
    "    # Dampen the weights close to the corners\n",
    "    dampening = dampening.lower()\n",
    "    if dampening == 'circular':\n",
    "        damping_map = circular_kernel(image.shape[1], image.shape[0])\n",
    "    elif dampening == 'gaussian':\n",
    "        damping_map = gaussian_kernel(image.shape[1], image.shape[0])\n",
    "    else:\n",
    "        damping_map = np.ones_like(image)\n",
    "    image = image * damping_map\n",
    "\n",
    "    # Apply Gaussian blur to smooth out the image\n",
    "    blurred_img = cv.GaussianBlur(image, (k_size, k_size), cv.BORDER_DEFAULT)\n",
    "\n",
    "    # Find the brightest spot\n",
    "    max_val = np.max(blurred_img)\n",
    "    max_idx = np.where(blurred_img == max_val)\n",
    "\n",
    "    # Average the brightest spots in case there are multiple\n",
    "    max_idx = np.mean(max_idx, axis=1)\n",
    "\n",
    "    # Find the centroid of the brightest spot\n",
    "    center_x = np.mean(max_idx[1])\n",
    "    center_y = np.mean(max_idx[0])\n",
    "\n",
    "    return int(center_x - width / 2), int(center_y - height / 2), width, height\n",
    "\n",
    "\n",
    "img = rgb_img.copy()\n",
    "x, y, w, h = brightest_spot(img, 512, 512, channel=-1, dampening='circular', k_size=65)\n",
    "cv.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 10)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b7293526288bfc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Method 3: Template Matching"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66654afe379feac5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the templates\n",
    "bounds = {\n",
    "    0: (800, 1300, 1350, 1750),\n",
    "    1: (650, 1150, 850, 1250),\n",
    "    2: (700, 1200, 750, 1250),\n",
    "    3: (750, 1250, 1400, 1850),\n",
    "    4: (770, 1220, 750, 1180),\n",
    "    20: (670, 1180, 650, 1100),\n",
    "    21: (600, 1100, 1050, 1500),\n",
    "    22: (720, 1170, 1070, 1500),\n",
    "    # 24\n",
    "}\n",
    "templates = []\n",
    "for idx, bound in bounds.items():\n",
    "    file = os.path.join(base_dir, files[idx])\n",
    "    bgr_img = cv.imread(file)\n",
    "    rgb_img = cv.cvtColor(bgr_img, cv.COLOR_BGR2RGB)\n",
    "    template = rgb_img.copy()[bound[0]:bound[1], bound[2]:bound[3], :]\n",
    "    templates.append(template)\n",
    "\n",
    "template = templates[-1]\n",
    "plt.imshow(template)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c95a5ef37ac379d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def template_matching(image, templates, margin_x: int = 50, margin_y: int = 50,\n",
    "                      scale: float = 1.0, min_confidence: float = 0.5, reduce: str = 'max'):\n",
    "    bboxes = []\n",
    "\n",
    "    # Resize image to smaller size for faster computation\n",
    "    if scale < 1.0:\n",
    "        image = cv.resize(image, (0, 0), fx=scale, fy=scale)\n",
    "\n",
    "    for template in templates:\n",
    "        # Resize template to smaller size\n",
    "        if scale < 1.0:\n",
    "            template = cv.resize(template, (0, 0), fx=scale, fy=scale)\n",
    "\n",
    "        # Safety check to make sure the template is smaller than the image\n",
    "        if image.shape[0] < template.shape[0] or image.shape[1] < template.shape[1]:\n",
    "            continue\n",
    "\n",
    "        # Apply template Matching\n",
    "        matched_image = cv.matchTemplate(image, template, cv.TM_CCOEFF_NORMED)\n",
    "        min_val, max_val, min_loc, max_loc = cv.minMaxLoc(matched_image)\n",
    "\n",
    "        # Get the bounding box\n",
    "        top_left_x, top_left_y = max_loc\n",
    "        height, width = template.shape[:2]\n",
    "\n",
    "        # Rescale back to original size\n",
    "        if scale < 1.0:\n",
    "            top_left_x = int(top_left_x / scale)\n",
    "            top_left_y = int(top_left_y / scale)\n",
    "            width = int(width / scale)\n",
    "            height = int(height / scale)\n",
    "\n",
    "        # Add margin to the bounding box\n",
    "        top_left_x -= margin_x\n",
    "        top_left_y -= margin_y\n",
    "        width += 2 * margin_x\n",
    "        height += 2 * margin_y\n",
    "\n",
    "        bboxes.append((top_left_x, top_left_y, width, height, max_val))\n",
    "\n",
    "    # Combine the bounding boxes\n",
    "    bboxes = np.array(bboxes)\n",
    "    # Sort by confidence\n",
    "    bboxes = bboxes[np.argsort(bboxes[:, 4])[::-1]]\n",
    "    # Remove low confidence bounding boxes\n",
    "    bboxes = bboxes[bboxes[:, 4] > min_confidence]\n",
    "\n",
    "    reduce = reduce.lower()\n",
    "    if reduce == 'mean':\n",
    "        return np.mean(bboxes, axis=0)[:4].astype(int)\n",
    "    elif reduce == 'median':\n",
    "        return np.median(bboxes, axis=0)[:4].astype(int)\n",
    "    elif reduce == 'max':\n",
    "        return np.max(bboxes, axis=0)[:4].astype(int)\n",
    "    elif reduce == 'min':\n",
    "        return np.min(bboxes, axis=0)[:4].astype(int)\n",
    "    elif reduce == 'join':\n",
    "        # Join the bounding boxes in extreme points\n",
    "        x_min = np.min(bboxes[:, 0])\n",
    "        y_min = np.min(bboxes[:, 1])\n",
    "        x_max = np.max(bboxes[:, 0] + bboxes[:, 2])\n",
    "        y_max = np.max(bboxes[:, 1] + bboxes[:, 3])\n",
    "        return np.array([x_min, y_min, x_max - x_min, y_max - y_min]).astype(int)\n",
    "    return bboxes[:, :4].astype(int)\n",
    "\n",
    "\n",
    "img = rgb_img.copy()\n",
    "x, y, w, h = template_matching(img, templates, margin_x=50, margin_y=50, scale=0.35, reduce='mean')\n",
    "img = cv.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 10)\n",
    "\n",
    "# res = template_matching(img, templates, margin_x=50, margin_y=50, scale=0.35, reduce='')\n",
    "# for x, y, w, h in res:\n",
    "#     img = cv.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 10)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b38a1771f4ca5a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Method 4: Thresholding channel"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aa3f68ca6e3e0d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import keep_largest_component, get_bounding_box\n",
    "\n",
    "\n",
    "def threshold_channel(image, margin_x: int = 50, margin_y: int = 50, channel: int = 2,\n",
    "                      crop_size: int | float = 256, k_size: int = 35):\n",
    "    # Crop the image to remove the black background\n",
    "    if isinstance(crop_size, float):\n",
    "        crop_size = int(crop_size * image.shape[0])\n",
    "    image = image[crop_size: -crop_size, crop_size: -crop_size, :]\n",
    "\n",
    "    # Get the specified channel (red, green, blue, or grey)\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2GRAY) if channel == -1 else image[..., channel]\n",
    "\n",
    "    # Otsu thresholding\n",
    "    _, image = cv.threshold(image, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "\n",
    "    # Dilate the mask\n",
    "    kernel = cv.getStructuringElement(cv.MORPH_RECT, (k_size, k_size))\n",
    "    image = cv.dilate(image, kernel)\n",
    "\n",
    "    # Get bounding box of largest connected component\n",
    "    image = keep_largest_component(image)\n",
    "    top_left_x, top_left_y, width, height = get_bounding_box(image)\n",
    "\n",
    "    top_left_x += crop_size - margin_x\n",
    "    top_left_y += crop_size - margin_y\n",
    "    width += 2 * margin_x\n",
    "    height += 2 * margin_y\n",
    "\n",
    "    return top_left_x, top_left_y, width, height\n",
    "\n",
    "\n",
    "img = rgb_img.copy()\n",
    "x, y, w, h = threshold_channel(img, channel=2, crop_size=200, k_size=35)\n",
    "cv.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 10)\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f06972afda92a17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing methods"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49616650c16786f2"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def portion_inside_bbox(binary_mask, bounding_box):\n",
    "    # Extract bounding box coordinates (x, y, w, h)\n",
    "    x, y, w, h = bounding_box\n",
    "\n",
    "    # Ensure the bounding box coordinates are within the bounds of the binary mask\n",
    "    x = max(0, x)\n",
    "    y = max(0, y)\n",
    "    w = min(w, binary_mask.shape[1] - x)\n",
    "    h = min(h, binary_mask.shape[0] - y)\n",
    "\n",
    "    # Extract the region of interest (ROI) from the binary mask\n",
    "    roi = binary_mask[y:y + h, x:x + w]\n",
    "\n",
    "    # Calculate the total number of 1s in the binary mask and the number of 1s in the ROI\n",
    "    total_ones = binary_mask[binary_mask > 0].sum()\n",
    "    ones_inside_bbox = roi[roi > 0].sum()\n",
    "\n",
    "    # Calculate the portion of 1s inside the bounding box\n",
    "    portion = ones_inside_bbox / total_ones if total_ones > 0 else 0.0\n",
    "\n",
    "    return portion\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T13:33:11.106143700Z",
     "start_time": "2023-09-20T13:33:11.086932400Z"
    }
   },
   "id": "7870defdda0622f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "percentages = defaultdict(list)\n",
    "areas = defaultdict(list)\n",
    "\n",
    "for file in files:\n",
    "    bgr_img = cv.imread(os.path.join(base_dir, file))\n",
    "    rgb_img = cv.cvtColor(bgr_img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    mask = cv.imread(os.path.join(base_mask_dir, file.replace('jpg', 'png')), cv.IMREAD_GRAYSCALE)\n",
    "    result = rgb_img.copy()\n",
    "    result[mask == 1] = (255, 255, 255)\n",
    "    result[mask == 2] = (0, 0, 0)\n",
    "\n",
    "    x, y, w, h = brightest_spot(rgb_img, 750, 750, channel=-1, dampening='circular', k_size=65)\n",
    "    percentages['brightest_spot'].append(portion_inside_bbox(mask, (x, y, w, h)))\n",
    "    cv.rectangle(result, (x, y), (x + w, y + h), (255, 0, 0), 10)\n",
    "    cv.putText(result, 'Brightest Spot', (10, 100), cv.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 10)\n",
    "\n",
    "    x, y, w, h = threshold_channel(rgb_img, channel=0, margin_x=50, margin_y=50, crop_size=250, k_size=35)\n",
    "    percentages['threshold_channel'].append(portion_inside_bbox(mask, (x, y, w, h)))\n",
    "    cv.rectangle(result, (x, y), (x + w, y + h), (0, 255, 0), 10)\n",
    "    cv.putText(result, 'Thresholding Channel', (10, 200), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 10)\n",
    "\n",
    "    x, y, w, h = template_matching(rgb_img, templates, margin_x=50, margin_y=50, scale=0.2, reduce='join')\n",
    "    percentages['template_matching_join'].append(portion_inside_bbox(mask, (x, y, w, h)))\n",
    "    cv.rectangle(result, (x, y), (x + w, y + h), (0, 0, 255), 10)\n",
    "    cv.putText(result, 'Template Matching (join)', (10, 300), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 10)\n",
    "\n",
    "    x, y, w, h = template_matching(rgb_img, templates, margin_x=50, margin_y=50, scale=0.2, reduce='mean')\n",
    "    percentages['template_matching_mean'].append(portion_inside_bbox(mask, (x, y, w, h)))\n",
    "    cv.rectangle(result, (x, y), (x + w, y + h), (255, 0, 255), 10)\n",
    "    cv.putText(result, 'Template Matching (mean)', (10, 400), cv.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 255), 10)\n",
    "\n",
    "    result = cv.cvtColor(result, cv.COLOR_RGB2BGR)\n",
    "    cv.imwrite(f'../logs/{file}', result)\n",
    "    print(file)\n",
    "\n",
    "    # if portion_inside_bbox(mask, (x, y, w, h)) < 1.0:\n",
    "    #     print('------------', file)\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.imshow(result)\n",
    "    # plt.show()\n",
    "    # break\n",
    "{key: np.mean(val) for key, val in percentages.items()}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2eae926468f1390"
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "data": {
      "text/plain": "{'brightest_spot': 0.9817020773760486,\n 'threshold_channel': 0.999190849862531,\n 'template_matching_join': 1.0,\n 'template_matching_mean': 0.9975996567031953}"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{key: np.mean(val) for key, val in percentages.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T18:54:31.750466900Z",
     "start_time": "2023-09-20T18:54:31.685625600Z"
    }
   },
   "id": "778ea7f51ff16fe0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:34:57.146261Z",
     "start_time": "2023-09-20T11:34:57.145266Z"
    }
   },
   "id": "51a50640fa9d8325"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
