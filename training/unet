digraph {
	graph [size="59.25,59.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2454388804880 [label="
 (1, 3, 128, 128)" fillcolor=darkolivegreen1]
	2454388763824 [label=ConvolutionBackward0]
	2454388761808 -> 2454388763824
	2454388761808 [label=ReluBackward0]
	2454389615552 -> 2454388761808
	2454389615552 [label=NativeBatchNormBackward0]
	2454389615648 -> 2454389615552
	2454389615648 [label=ConvolutionBackward0]
	2454389615216 -> 2454389615648
	2454389615216 [label=ReluBackward0]
	2454389614832 -> 2454389615216
	2454389614832 [label=NativeBatchNormBackward0]
	2454389612528 -> 2454389614832
	2454389612528 [label=ConvolutionBackward0]
	2454389614592 -> 2454389612528
	2454389614592 [label=CatBackward0]
	2454389615408 -> 2454389614592
	2454389615408 [label=ReluBackward0]
	2454389611664 -> 2454389615408
	2454389611664 [label=NativeBatchNormBackward0]
	2454389612288 -> 2454389611664
	2454389612288 [label=ConvolutionBackward0]
	2454389615792 -> 2454389612288
	2454389615792 [label=ReluBackward0]
	2454389615936 -> 2454389615792
	2454389615936 [label=NativeBatchNormBackward0]
	2454389616032 -> 2454389615936
	2454389616032 [label=ConvolutionBackward0]
	2454389616224 -> 2454389616032
	2454388953376 [label="encoder.0.conv.0.weight
 (32, 3, 3, 3)" fillcolor=lightblue]
	2454388953376 -> 2454389616224
	2454389616224 [label=AccumulateGrad]
	2454389615984 -> 2454389615936
	2454388959376 [label="encoder.0.conv.1.weight
 (32)" fillcolor=lightblue]
	2454388959376 -> 2454389615984
	2454389615984 [label=AccumulateGrad]
	2454389615840 -> 2454389615936
	2454388952016 [label="encoder.0.conv.1.bias
 (32)" fillcolor=lightblue]
	2454388952016 -> 2454389615840
	2454389615840 [label=AccumulateGrad]
	2454389612768 -> 2454389612288
	2454389017952 [label="encoder.0.conv.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2454389017952 -> 2454389612768
	2454389612768 [label=AccumulateGrad]
	2454389612576 -> 2454389611664
	2454389018032 [label="encoder.0.conv.4.weight
 (32)" fillcolor=lightblue]
	2454389018032 -> 2454389612576
	2454389612576 [label=AccumulateGrad]
	2454389615360 -> 2454389611664
	2454389018112 [label="encoder.0.conv.4.bias
 (32)" fillcolor=lightblue]
	2454389018112 -> 2454389615360
	2454389615360 [label=AccumulateGrad]
	2454389615456 -> 2454389614592
	2454389615456 [label=ConvolutionBackward0]
	2454389612720 -> 2454389615456
	2454389612720 [label=ReluBackward0]
	2454389616272 -> 2454389612720
	2454389616272 [label=NativeBatchNormBackward0]
	2454389616368 -> 2454389616272
	2454389616368 [label=ConvolutionBackward0]
	2454389616560 -> 2454389616368
	2454389616560 [label=ReluBackward0]
	2454389616704 -> 2454389616560
	2454389616704 [label=NativeBatchNormBackward0]
	2454389616800 -> 2454389616704
	2454389616800 [label=ConvolutionBackward0]
	2454389616992 -> 2454389616800
	2454389616992 [label=CatBackward0]
	2454389617136 -> 2454389616992
	2454389617136 [label=ReluBackward0]
	2454389617280 -> 2454389617136
	2454389617280 [label=NativeBatchNormBackward0]
	2454389617376 -> 2454389617280
	2454389617376 [label=ConvolutionBackward0]
	2454389617568 -> 2454389617376
	2454389617568 [label=ReluBackward0]
	2454389617712 -> 2454389617568
	2454389617712 [label=NativeBatchNormBackward0]
	2454389617808 -> 2454389617712
	2454389617808 [label=ConvolutionBackward0]
	2454389618000 -> 2454389617808
	2454389618000 [label=MaxPool2DWithIndicesBackward0]
	2454389615408 -> 2454389618000
	2454389617952 -> 2454389617808
	2454389018512 [label="encoder.1.conv.0.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	2454389018512 -> 2454389617952
	2454389617952 [label=AccumulateGrad]
	2454389617760 -> 2454389617712
	2454389018592 [label="encoder.1.conv.1.weight
 (64)" fillcolor=lightblue]
	2454389018592 -> 2454389617760
	2454389617760 [label=AccumulateGrad]
	2454389617616 -> 2454389617712
	2454389018672 [label="encoder.1.conv.1.bias
 (64)" fillcolor=lightblue]
	2454389018672 -> 2454389617616
	2454389617616 [label=AccumulateGrad]
	2454389617520 -> 2454389617376
	2454389019152 [label="encoder.1.conv.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2454389019152 -> 2454389617520
	2454389617520 [label=AccumulateGrad]
	2454389617328 -> 2454389617280
	2454389019232 [label="encoder.1.conv.4.weight
 (64)" fillcolor=lightblue]
	2454389019232 -> 2454389617328
	2454389617328 [label=AccumulateGrad]
	2454389617184 -> 2454389617280
	2454389019312 [label="encoder.1.conv.4.bias
 (64)" fillcolor=lightblue]
	2454389019312 -> 2454389617184
	2454389617184 [label=AccumulateGrad]
	2454389617088 -> 2454389616992
	2454389617088 [label=ConvolutionBackward0]
	2454389617472 -> 2454389617088
	2454389617472 [label=ReluBackward0]
	2454389618192 -> 2454389617472
	2454389618192 [label=NativeBatchNormBackward0]
	2454389618144 -> 2454389618192
	2454389618144 [label=ConvolutionBackward0]
	2454389618384 -> 2454389618144
	2454389618384 [label=ReluBackward0]
	2454389618528 -> 2454389618384
	2454389618528 [label=NativeBatchNormBackward0]
	2454389618624 -> 2454389618528
	2454389618624 [label=ConvolutionBackward0]
	2454389618816 -> 2454389618624
	2454389618816 [label=CatBackward0]
	2454389618960 -> 2454389618816
	2454389618960 [label=ReluBackward0]
	2454389619104 -> 2454389618960
	2454389619104 [label=NativeBatchNormBackward0]
	2454389619200 -> 2454389619104
	2454389619200 [label=ConvolutionBackward0]
	2454389619392 -> 2454389619200
	2454389619392 [label=ReluBackward0]
	2454389619536 -> 2454389619392
	2454389619536 [label=NativeBatchNormBackward0]
	2454389619632 -> 2454389619536
	2454389619632 [label=ConvolutionBackward0]
	2454389619824 -> 2454389619632
	2454389619824 [label=MaxPool2DWithIndicesBackward0]
	2454389617136 -> 2454389619824
	2454389619776 -> 2454389619632
	2454389019792 [label="encoder.2.conv.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2454389019792 -> 2454389619776
	2454389619776 [label=AccumulateGrad]
	2454389619584 -> 2454389619536
	2454389019872 [label="encoder.2.conv.1.weight
 (128)" fillcolor=lightblue]
	2454389019872 -> 2454389619584
	2454389619584 [label=AccumulateGrad]
	2454389619440 -> 2454389619536
	2454389019952 [label="encoder.2.conv.1.bias
 (128)" fillcolor=lightblue]
	2454389019952 -> 2454389619440
	2454389619440 [label=AccumulateGrad]
	2454389619344 -> 2454389619200
	2454389020432 [label="encoder.2.conv.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2454389020432 -> 2454389619344
	2454389619344 [label=AccumulateGrad]
	2454389619152 -> 2454389619104
	2454389020512 [label="encoder.2.conv.4.weight
 (128)" fillcolor=lightblue]
	2454389020512 -> 2454389619152
	2454389619152 [label=AccumulateGrad]
	2454389619008 -> 2454389619104
	2454389020592 [label="encoder.2.conv.4.bias
 (128)" fillcolor=lightblue]
	2454389020592 -> 2454389619008
	2454389619008 [label=AccumulateGrad]
	2454389618912 -> 2454389618816
	2454389618912 [label=ConvolutionBackward0]
	2454389619296 -> 2454389618912
	2454389619296 [label=ReluBackward0]
	2454389620064 -> 2454389619296
	2454389620064 [label=NativeBatchNormBackward0]
	2454389619968 -> 2454389620064
	2454389619968 [label=ConvolutionBackward0]
	2454389620256 -> 2454389619968
	2454389620256 [label=ReluBackward0]
	2454389620400 -> 2454389620256
	2454389620400 [label=NativeBatchNormBackward0]
	2454389620496 -> 2454389620400
	2454389620496 [label=ConvolutionBackward0]
	2454389620688 -> 2454389620496
	2454389620688 [label=CatBackward0]
	2454389866656 -> 2454389620688
	2454389866656 [label=ReluBackward0]
	2454389866800 -> 2454389866656
	2454389866800 [label=NativeBatchNormBackward0]
	2454389866896 -> 2454389866800
	2454389866896 [label=ConvolutionBackward0]
	2454389867088 -> 2454389866896
	2454389867088 [label=ReluBackward0]
	2454389867232 -> 2454389867088
	2454389867232 [label=NativeBatchNormBackward0]
	2454389867328 -> 2454389867232
	2454389867328 [label=ConvolutionBackward0]
	2454389867520 -> 2454389867328
	2454389867520 [label=MaxPool2DWithIndicesBackward0]
	2454389618960 -> 2454389867520
	2454389867472 -> 2454389867328
	2454389021072 [label="encoder.3.conv.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2454389021072 -> 2454389867472
	2454389867472 [label=AccumulateGrad]
	2454389867280 -> 2454389867232
	2454389021152 [label="encoder.3.conv.1.weight
 (256)" fillcolor=lightblue]
	2454389021152 -> 2454389867280
	2454389867280 [label=AccumulateGrad]
	2454389867136 -> 2454389867232
	2454389021232 [label="encoder.3.conv.1.bias
 (256)" fillcolor=lightblue]
	2454389021232 -> 2454389867136
	2454389867136 [label=AccumulateGrad]
	2454389867040 -> 2454389866896
	2454389021712 [label="encoder.3.conv.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2454389021712 -> 2454389867040
	2454389867040 [label=AccumulateGrad]
	2454389866848 -> 2454389866800
	2454389021792 [label="encoder.3.conv.4.weight
 (256)" fillcolor=lightblue]
	2454389021792 -> 2454389866848
	2454389866848 [label=AccumulateGrad]
	2454389866704 -> 2454389866800
	2454389021872 [label="encoder.3.conv.4.bias
 (256)" fillcolor=lightblue]
	2454389021872 -> 2454389866704
	2454389866704 [label=AccumulateGrad]
	2454389866608 -> 2454389620688
	2454389866608 [label=ConvolutionBackward0]
	2454389866992 -> 2454389866608
	2454389866992 [label=ReluBackward0]
	2454389867760 -> 2454389866992
	2454389867760 [label=NativeBatchNormBackward0]
	2454389867664 -> 2454389867760
	2454389867664 [label=ConvolutionBackward0]
	2454389867952 -> 2454389867664
	2454389867952 [label=ReluBackward0]
	2454389868096 -> 2454389867952
	2454389868096 [label=NativeBatchNormBackward0]
	2454389868192 -> 2454389868096
	2454389868192 [label=ConvolutionBackward0]
	2454389868384 -> 2454389868192
	2454389868384 [label=MaxPool2DWithIndicesBackward0]
	2454389866656 -> 2454389868384
	2454389868336 -> 2454389868192
	2454389022272 [label="bottleneck.conv.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2454389022272 -> 2454389868336
	2454389868336 [label=AccumulateGrad]
	2454389868144 -> 2454389868096
	2454389022352 [label="bottleneck.conv.1.weight
 (512)" fillcolor=lightblue]
	2454389022352 -> 2454389868144
	2454389868144 [label=AccumulateGrad]
	2454389868000 -> 2454389868096
	2454389022432 [label="bottleneck.conv.1.bias
 (512)" fillcolor=lightblue]
	2454389022432 -> 2454389868000
	2454389868000 [label=AccumulateGrad]
	2454389867904 -> 2454389867664
	2454389022912 [label="bottleneck.conv.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2454389022912 -> 2454389867904
	2454389867904 [label=AccumulateGrad]
	2454389867712 -> 2454389867760
	2454389022992 [label="bottleneck.conv.4.weight
 (512)" fillcolor=lightblue]
	2454389022992 -> 2454389867712
	2454389867712 [label=AccumulateGrad]
	2454389867376 -> 2454389867760
	2454389023072 [label="bottleneck.conv.4.bias
 (512)" fillcolor=lightblue]
	2454389023072 -> 2454389867376
	2454389867376 [label=AccumulateGrad]
	2454389866944 -> 2454389866608
	2454389023472 [label="decoder.0.weight
 (512, 256, 2, 2)" fillcolor=lightblue]
	2454389023472 -> 2454389866944
	2454389866944 [label=AccumulateGrad]
	2454389866752 -> 2454389866608
	2454389023552 [label="decoder.0.bias
 (256)" fillcolor=lightblue]
	2454389023552 -> 2454389866752
	2454389866752 [label=AccumulateGrad]
	2454389620640 -> 2454389620496
	2454389023712 [label="decoder.1.conv.0.weight
 (256, 512, 3, 3)" fillcolor=lightblue]
	2454389023712 -> 2454389620640
	2454389620640 [label=AccumulateGrad]
	2454389620448 -> 2454389620400
	2454389023792 [label="decoder.1.conv.1.weight
 (256)" fillcolor=lightblue]
	2454389023792 -> 2454389620448
	2454389620448 [label=AccumulateGrad]
	2454389620304 -> 2454389620400
	2454389023872 [label="decoder.1.conv.1.bias
 (256)" fillcolor=lightblue]
	2454389023872 -> 2454389620304
	2454389620304 [label=AccumulateGrad]
	2454389620208 -> 2454389619968
	2454389024352 [label="decoder.1.conv.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2454389024352 -> 2454389620208
	2454389620208 [label=AccumulateGrad]
	2454389620016 -> 2454389620064
	2454389024432 [label="decoder.1.conv.4.weight
 (256)" fillcolor=lightblue]
	2454389024432 -> 2454389620016
	2454389620016 [label=AccumulateGrad]
	2454389619680 -> 2454389620064
	2454389024512 [label="decoder.1.conv.4.bias
 (256)" fillcolor=lightblue]
	2454389024512 -> 2454389619680
	2454389619680 [label=AccumulateGrad]
	2454389619248 -> 2454389618912
	2454389024912 [label="decoder.2.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	2454389024912 -> 2454389619248
	2454389619248 [label=AccumulateGrad]
	2454389619056 -> 2454389618912
	2454389024992 [label="decoder.2.bias
 (128)" fillcolor=lightblue]
	2454389024992 -> 2454389619056
	2454389619056 [label=AccumulateGrad]
	2454389618768 -> 2454389618624
	2454389025152 [label="decoder.3.conv.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	2454389025152 -> 2454389618768
	2454389618768 [label=AccumulateGrad]
	2454389618576 -> 2454389618528
	2454389025232 [label="decoder.3.conv.1.weight
 (128)" fillcolor=lightblue]
	2454389025232 -> 2454389618576
	2454389618576 [label=AccumulateGrad]
	2454389618432 -> 2454389618528
	2454389025312 [label="decoder.3.conv.1.bias
 (128)" fillcolor=lightblue]
	2454389025312 -> 2454389618432
	2454389618432 [label=AccumulateGrad]
	2454389618336 -> 2454389618144
	2454389025792 [label="decoder.3.conv.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2454389025792 -> 2454389618336
	2454389618336 [label=AccumulateGrad]
	2454389618048 -> 2454389618192
	2454389025872 [label="decoder.3.conv.4.weight
 (128)" fillcolor=lightblue]
	2454389025872 -> 2454389618048
	2454389618048 [label=AccumulateGrad]
	2454389617856 -> 2454389618192
	2454389025952 [label="decoder.3.conv.4.bias
 (128)" fillcolor=lightblue]
	2454389025952 -> 2454389617856
	2454389617856 [label=AccumulateGrad]
	2454389617424 -> 2454389617088
	2454389026432 [label="decoder.4.weight
 (128, 64, 2, 2)" fillcolor=lightblue]
	2454389026432 -> 2454389617424
	2454389617424 [label=AccumulateGrad]
	2454389617232 -> 2454389617088
	2454389026512 [label="decoder.4.bias
 (64)" fillcolor=lightblue]
	2454389026512 -> 2454389617232
	2454389617232 [label=AccumulateGrad]
	2454389616944 -> 2454389616800
	2454389026672 [label="decoder.5.conv.0.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	2454389026672 -> 2454389616944
	2454389616944 [label=AccumulateGrad]
	2454389616752 -> 2454389616704
	2454389026752 [label="decoder.5.conv.1.weight
 (64)" fillcolor=lightblue]
	2454389026752 -> 2454389616752
	2454389616752 [label=AccumulateGrad]
	2454389616608 -> 2454389616704
	2454389026832 [label="decoder.5.conv.1.bias
 (64)" fillcolor=lightblue]
	2454389026832 -> 2454389616608
	2454389616608 [label=AccumulateGrad]
	2454389616512 -> 2454389616368
	2454389027312 [label="decoder.5.conv.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2454389027312 -> 2454389616512
	2454389616512 [label=AccumulateGrad]
	2454389616128 -> 2454389616272
	2454389027392 [label="decoder.5.conv.4.weight
 (64)" fillcolor=lightblue]
	2454389027392 -> 2454389616128
	2454389616128 [label=AccumulateGrad]
	2454389616080 -> 2454389616272
	2454389027472 [label="decoder.5.conv.4.bias
 (64)" fillcolor=lightblue]
	2454389027472 -> 2454389616080
	2454389616080 [label=AccumulateGrad]
	2454389612192 -> 2454389615456
	2454389027792 [label="decoder.6.weight
 (64, 32, 2, 2)" fillcolor=lightblue]
	2454389027792 -> 2454389612192
	2454389612192 [label=AccumulateGrad]
	2454389615312 -> 2454389615456
	2454389027872 [label="decoder.6.bias
 (32)" fillcolor=lightblue]
	2454389027872 -> 2454389615312
	2454389615312 [label=AccumulateGrad]
	2454389614640 -> 2454389612528
	2454389028032 [label="decoder.7.conv.0.weight
 (32, 64, 3, 3)" fillcolor=lightblue]
	2454389028032 -> 2454389614640
	2454389614640 [label=AccumulateGrad]
	2454389614880 -> 2454389614832
	2454389028112 [label="decoder.7.conv.1.weight
 (32)" fillcolor=lightblue]
	2454389028112 -> 2454389614880
	2454389614880 [label=AccumulateGrad]
	2454389615264 -> 2454389614832
	2454389028192 [label="decoder.7.conv.1.bias
 (32)" fillcolor=lightblue]
	2454389028192 -> 2454389615264
	2454389615264 [label=AccumulateGrad]
	2454389615168 -> 2454389615648
	2454389028672 [label="decoder.7.conv.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2454389028672 -> 2454389615168
	2454389615168 [label=AccumulateGrad]
	2454389615600 -> 2454389615552
	2454389028752 [label="decoder.7.conv.4.weight
 (32)" fillcolor=lightblue]
	2454389028752 -> 2454389615600
	2454389615600 [label=AccumulateGrad]
	2454389614736 -> 2454389615552
	2454389028832 [label="decoder.7.conv.4.bias
 (32)" fillcolor=lightblue]
	2454389028832 -> 2454389614736
	2454389614736 [label=AccumulateGrad]
	2454389615120 -> 2454388763824
	2454389029312 [label="output.weight
 (3, 32, 1, 1)" fillcolor=lightblue]
	2454389029312 -> 2454389615120
	2454389615120 [label=AccumulateGrad]
	2454389615072 -> 2454388763824
	2454389029392 [label="output.bias
 (3)" fillcolor=lightblue]
	2454389029392 -> 2454389615072
	2454389615072 [label=AccumulateGrad]
	2454388763824 -> 2454388804880
}
